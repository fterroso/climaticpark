{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BooWVfJkNRfs",
    "outputId": "461d7b73-52f7-48c4-f3d0-26ca621a4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybdshadow\n",
      "  Downloading pybdshadow-0.3.5-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting contextily\n",
      "  Downloading contextily-1.6.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
      "Collecting timezonefinder\n",
      "  Downloading timezonefinder-6.5.8-cp311-cp311-manylinux_2_17_x86_64.manylinux_2_5_x86_64.manylinux1_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pybdshadow) (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pybdshadow) (2.2.2)\n",
      "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from pybdshadow) (2.0.7)\n",
      "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (from pybdshadow) (1.0.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pybdshadow) (3.10.0)\n",
      "Collecting suncalc (from pybdshadow)\n",
      "  Downloading suncalc-0.1.3.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting keplergl (from pybdshadow)\n",
      "  Downloading keplergl-0.3.7.tar.gz (18.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.4/18.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting transbigdata (from pybdshadow)\n",
      "  Downloading transbigdata-0.5.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting mapbox-vector-tile (from pybdshadow)\n",
      "  Downloading mapbox_vector_tile-2.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting vt2geojson (from pybdshadow)\n",
      "  Downloading vt2geojson-0.2.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pybdshadow) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pybdshadow) (4.67.1)\n",
      "Collecting retrying (from pybdshadow)\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (from contextily) (2.4.1)\n",
      "Collecting mercantile (from contextily)\n",
      "  Downloading mercantile-1.2.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting rasterio (from contextily)\n",
      "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from contextily) (1.4.2)\n",
      "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from contextily) (2025.1.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.5)\n",
      "Requirement already satisfied: cffi<2,>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from timezonefinder) (1.17.1)\n",
      "Collecting h3>4 (from timezonefinder)\n",
      "  Downloading h3-4.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2,>=1.15.1->timezonefinder) (2.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas->pybdshadow) (0.10.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas->pybdshadow) (3.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pybdshadow) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pybdshadow) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pybdshadow) (2025.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy->contextily) (2.0)\n",
      "Collecting ipywidgets>=8.1.5 (from keplergl->pybdshadow)\n",
      "  Using cached ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: traittypes>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from keplergl->pybdshadow) (0.2.1)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.11/dist-packages (from keplergl->pybdshadow) (5.7.1)\n",
      "Collecting jupyter_packaging>=0.12.3 (from keplergl->pybdshadow)\n",
      "  Using cached jupyter_packaging-0.12.3-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting jupyter>=1.0.0 (from keplergl->pybdshadow)\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting jupyterlab>=4.1.6 (from keplergl->pybdshadow)\n",
      "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: notebook>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from keplergl->pybdshadow) (6.5.5)\n",
      "Requirement already satisfied: pyarrow>=16.0.0 in /usr/local/lib/python3.11/dist-packages (from keplergl->pybdshadow) (17.0.0)\n",
      "Collecting geoarrow-pyarrow>=0.1.2 (from keplergl->pybdshadow)\n",
      "  Downloading geoarrow_pyarrow-0.1.2-py3-none-any.whl.metadata (613 bytes)\n",
      "Collecting geoarrow-pandas>=0.1.1 (from keplergl->pybdshadow)\n",
      "  Downloading geoarrow_pandas-0.1.1-py3-none-any.whl.metadata (493 bytes)\n",
      "Collecting protobuf<6.0.0,>=5.26.1 (from mapbox-vector-tile->pybdshadow)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pyclipper<2.0.0,>=1.3.0 (from mapbox-vector-tile->pybdshadow)\n",
      "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybdshadow) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybdshadow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybdshadow) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybdshadow) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybdshadow) (3.2.1)\n",
      "Requirement already satisfied: click>=3.0 in /usr/local/lib/python3.11/dist-packages (from mercantile->contextily) (8.1.8)\n",
      "Collecting affine (from rasterio->contextily)\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio->contextily) (25.1.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio->contextily) (2025.1.31)\n",
      "Collecting cligj>=0.5 (from rasterio->contextily)\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting click-plugins (from rasterio->contextily)\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pybdshadow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pybdshadow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pybdshadow) (2.3.0)\n",
      "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from retrying->pybdshadow) (1.17.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from transbigdata->pybdshadow) (1.13.1)\n",
      "Collecting pykalman (from transbigdata->pybdshadow)\n",
      "  Downloading pykalman-0.10.1-py2.py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting osmnx (from transbigdata->pybdshadow)\n",
      "  Downloading osmnx-2.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting geoarrow-c (from geoarrow-pyarrow>=0.1.2->keplergl->pybdshadow)\n",
      "  Downloading geoarrow_c-0.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (400 bytes)\n",
      "Collecting pyarrow-hotfix (from geoarrow-pyarrow>=0.1.2->keplergl->pybdshadow)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting comm>=0.1.3 (from ipywidgets>=8.1.5->keplergl->pybdshadow)\n",
      "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.1.5->keplergl->pybdshadow) (7.34.0)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.1.5->keplergl->pybdshadow)\n",
      "  Using cached widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.1.5->keplergl->pybdshadow) (3.0.13)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter>=1.0.0->keplergl->pybdshadow) (6.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter>=1.0.0->keplergl->pybdshadow) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter>=1.0.0->keplergl->pybdshadow) (6.17.1)\n",
      "Collecting deprecation (from jupyter_packaging>=0.12.3->keplergl->pybdshadow)\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools>=60.2.0 in /usr/local/lib/python3.11/dist-packages (from jupyter_packaging>=0.12.3->keplergl->pybdshadow) (75.1.0)\n",
      "Collecting tomlkit (from jupyter_packaging>=0.12.3->keplergl->pybdshadow)\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from jupyter_packaging>=0.12.3->keplergl->pybdshadow) (0.45.1)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=4.1.6->keplergl->pybdshadow) (0.28.1)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=4.1.6->keplergl->pybdshadow) (5.7.2)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=4.1.6->keplergl->pybdshadow) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab>=4.1.6->keplergl->pybdshadow) (6.4.2)\n",
      "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (24.0.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (6.1.12)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (0.2.0)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (5.10.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (1.6.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (0.21.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0.1->keplergl->pybdshadow) (1.2.0)\n",
      "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from osmnx->transbigdata->pybdshadow) (3.4.2)\n",
      "Collecting scikit-base<0.13.0 (from pykalman->transbigdata->pybdshadow)\n",
      "  Downloading scikit_base-0.12.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab>=4.1.6->keplergl->pybdshadow) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab>=4.1.6->keplergl->pybdshadow) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab>=4.1.6->keplergl->pybdshadow) (0.14.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter>=1.0.0->keplergl->pybdshadow) (1.8.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter>=1.0.0->keplergl->pybdshadow) (0.1.7)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter>=1.0.0->keplergl->pybdshadow) (5.9.5)\n",
      "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow)\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow) (3.0.50)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow) (2.18.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab>=4.1.6->keplergl->pybdshadow) (4.3.6)\n",
      "Collecting jupyter-client<8,>=5.3.4 (from notebook>=6.0.1->keplergl->pybdshadow)\n",
      "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=6.0.1->keplergl->pybdshadow) (21.2.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client<8,>=5.3.4->notebook>=6.0.1->keplergl->pybdshadow) (0.4)\n",
      "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=4.1.6->keplergl->pybdshadow) (2.17.0)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=4.1.6->keplergl->pybdshadow) (4.23.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (4.13.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (3.1.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (1.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=6.0.1->keplergl->pybdshadow) (2.21.1)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook>=6.0.1->keplergl->pybdshadow) (0.7.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab>=4.1.6->keplergl->pybdshadow) (1.3.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (1.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow) (0.8.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=4.1.6->keplergl->pybdshadow) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=4.1.6->keplergl->pybdshadow) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=4.1.6->keplergl->pybdshadow) (0.22.3)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow) (6.0.2)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.1.5->keplergl->pybdshadow) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->keplergl->pybdshadow) (4.12.2)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow) (3.0.0)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow) (24.11.1)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl->pybdshadow)\n",
      "  Using cached types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading pybdshadow-0.3.5-py3-none-any.whl (34 kB)\n",
      "Downloading contextily-1.6.2-py3-none-any.whl (17 kB)\n",
      "Downloading timezonefinder-6.5.8-cp311-cp311-manylinux_2_17_x86_64.manylinux_2_5_x86_64.manylinux1_x86_64.manylinux2014_x86_64.whl (51.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h3-4.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mapbox_vector_tile-2.1.0-py3-none-any.whl (30 kB)\n",
      "Downloading mercantile-1.2.1-py3-none-any.whl (14 kB)\n",
      "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading transbigdata-0.5.3-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading vt2geojson-0.2.1-py3-none-any.whl (7.0 kB)\n",
      "Using cached cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading geoarrow_pandas-0.1.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading geoarrow_pyarrow-0.1.2-py3-none-any.whl (26 kB)\n",
      "Using cached ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Using cached jupyter_packaging-0.12.3-py3-none-any.whl (15 kB)\n",
      "Downloading jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading osmnx-2.0.1-py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.6/99.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pykalman-0.10.1-py2.py3-none-any.whl (248 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Using cached comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Using cached jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "Using cached jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
      "Downloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Downloading scikit_base-0.12.0-py3-none-any.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.5/141.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading geoarrow_c-0.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached json5-0.10.0-py3-none-any.whl (34 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: keplergl, suncalc\n",
      "  Building wheel for keplergl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keplergl: filename=keplergl-0.3.7-py2.py3-none-any.whl size=35080505 sha256=175cd0ff3d8f4114a2f418bbb9eed296e1fbdbc46394ac18acf1b9c6fbeb13dc\n",
      "  Stored in directory: /root/.cache/pip/wheels/c4/15/4b/f54140da2477a3d3d74bab2fccfc495849a1b3f2ca391f7c18\n",
      "  Building wheel for suncalc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for suncalc: filename=suncalc-0.1.3-py3-none-any.whl size=10221 sha256=50418a0424f74edd3e4c3c644349a9d50dc50d230c028fa30b2f6239b877e787\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/76/83/b3512ab84cbf25b81b6455cfe58afbaf326aae010f476ac863\n",
      "Successfully built keplergl suncalc\n",
      "Installing collected packages: pyclipper, widgetsnbextension, uri-template, types-python-dateutil, tomlkit, suncalc, scikit-base, rfc3986-validator, rfc3339-validator, retrying, python-json-logger, pyarrow-hotfix, protobuf, overrides, mercantile, json5, jedi, h3, geoarrow-c, fqdn, deprecation, comm, cligj, click-plugins, async-lru, affine, timezonefinder, rasterio, pykalman, mapbox-vector-tile, jupyter-server-terminals, jupyter_packaging, jupyter-client, geoarrow-pyarrow, arrow, vt2geojson, isoduration, ipywidgets, geoarrow-pandas, contextily, osmnx, transbigdata, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, keplergl, pybdshadow\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.6.10\n",
      "    Uninstalling widgetsnbextension-3.6.10:\n",
      "      Successfully uninstalled widgetsnbextension-3.6.10\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.6\n",
      "    Uninstalling protobuf-4.25.6:\n",
      "      Successfully uninstalled protobuf-4.25.6\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter-client 6.1.12\n",
      "    Uninstalling jupyter-client-6.1.12:\n",
      "      Successfully uninstalled jupyter-client-6.1.12\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.7.1\n",
      "    Uninstalling ipywidgets-7.7.1:\n",
      "      Successfully uninstalled ipywidgets-7.7.1\n",
      "  Attempting uninstall: jupyter-server\n",
      "    Found existing installation: jupyter-server 1.24.0\n",
      "    Uninstalling jupyter-server-1.24.0:\n",
      "      Successfully uninstalled jupyter-server-1.24.0\n",
      "Successfully installed affine-2.4.0 arrow-1.3.0 async-lru-2.0.4 click-plugins-1.1.1 cligj-0.7.2 comm-0.2.2 contextily-1.6.2 deprecation-2.1.0 fqdn-1.5.1 geoarrow-c-0.1.2 geoarrow-pandas-0.1.1 geoarrow-pyarrow-0.1.2 h3-4.2.1 ipywidgets-8.1.5 isoduration-20.11.0 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyter_packaging-0.12.3 jupyterlab-4.3.5 jupyterlab-server-2.27.3 keplergl-0.3.7 mapbox-vector-tile-2.1.0 mercantile-1.2.1 osmnx-2.0.1 overrides-7.7.0 protobuf-5.29.3 pyarrow-hotfix-0.6 pybdshadow-0.3.5 pyclipper-1.3.0.post6 pykalman-0.10.1 python-json-logger-3.2.1 rasterio-1.4.3 retrying-1.3.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 scikit-base-0.12.0 suncalc-0.1.3 timezonefinder-6.5.8 tomlkit-0.13.2 transbigdata-0.5.3 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0 vt2geojson-0.2.1 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries (see configuration file .yml in github repo)\n",
    "#!pip install pybdshadow contextily folium pillow timezonefinder plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Gw2SxXJJNyhf"
   },
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import pytz\n",
    "from timezonefinder import TimezoneFinder\n",
    "import json\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "import plotly.express as px\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShadowModule:\n",
    "        \n",
    "        def __init__(self,lat, lon, roofs, spaces):\n",
    "            self.lat= lat\n",
    "            self.lon= lon\n",
    "            self.roofs = roofs\n",
    "            self.spaces= spaces\n",
    "        \n",
    "        \n",
    "        def compute_coverage_rates(self, days_lst):\n",
    "            \"\"\"\n",
    "            Compute coverage rates for each day in days_lst\n",
    "            \"\"\"\n",
    "\n",
    "            # DataFrames to store results\n",
    "            coverage_rates_df = pd.DataFrame()\n",
    "\n",
    "            for current_date in days_lst:\n",
    "                # Compute shadow projections for roofs\n",
    "                shadows = self._all_sunshadeshadow_sunlight(current_date)\n",
    "\n",
    "                # Calculate coverage rates\n",
    "                coverage_rates = []\n",
    "                for index, parking_space in self.spaces.iterrows():\n",
    "                    parking_space_gdf = gpd.GeoDataFrame(geometry=[parking_space.geometry])\n",
    "                    parking_space_gdf = parking_space_gdf.set_crs(epsg=4326)\n",
    "                    parking_space_gdf = parking_space_gdf.to_crs(epsg=shadows.crs.to_epsg())\n",
    "\n",
    "                    intersection = gpd.overlay(parking_space_gdf, shadows, how='intersection')\n",
    "\n",
    "                    intersection_area = intersection.geometry.area.sum()\n",
    "                    parking_space_area = parking_space_gdf.geometry.area.sum()\n",
    "\n",
    "                    coverage_rate = intersection_area / parking_space_area\n",
    "                    coverage_rates.append(coverage_rate)\n",
    "\n",
    "                coverage_rates_df[f'coverage_rate_{current_date.strftime(\"%Y-%m-%d %H:%M:%S\")}'] = coverage_rates\n",
    "\n",
    "            return coverage_rates_df\n",
    "\n",
    "            # Define function to calculate shadow and sunlight for all rooftops\n",
    "        def _all_sunshadeshadow_sunlight(date):\n",
    "            roof_projected_df= self.roofs\n",
    "            roof_projected_df['geometry'] = roof_projected_df.apply(lambda r: self._sunshadeshadow_sunlight(date, r[0]), axis=1)\n",
    "            return roof_projected_df\n",
    "    \n",
    "    \n",
    "        def _sunshadeshadow_sunlight(date, r, sunshade_height=2):\n",
    "            meanlon= r.centroid.x\n",
    "            meanlat= r.centroid.y\n",
    "            # obtain sun position\n",
    "            sunPosition = get_position(date, meanlon, meanlat)\n",
    "            if sunPosition['altitude'] < 0:\n",
    "                raise ValueError(\"Given time before sunrise or after sunset\")\n",
    "                \n",
    "            r_coords= np.array(r.exterior.coords)\n",
    "            r_coords= r_coords.reshape(1,-1,2)\n",
    "            shape = pybdshadow.utils.lonlat2aeqd(r_coords,meanlon,meanlat)\n",
    "            azimuth = sunPosition['azimuth']\n",
    "            altitude = sunPosition['altitude']\n",
    "\n",
    "            n = np.shape(shape)[0]\n",
    "            distance = sunshade_height / math.tan(altitude)\n",
    "\n",
    "            # calculate the offset of the projection position\n",
    "            lonDistance = distance * math.sin(azimuth)\n",
    "            latDistance = distance * math.cos(azimuth)\n",
    "\n",
    "            shadowShape = np.zeros((1, 5, 2))\n",
    "            shadowShape[:, :, :] += shape\n",
    "            shadowShape[:, :, 0] = shape[:, :, 0] + lonDistance\n",
    "            shadowShape[:, :, 1] = shape[:, :, 1] + latDistance\n",
    "            shadowShape = pybdshadow.utils.aeqd2lonlat(shadowShape,meanlon,meanlat)\n",
    "            p = Polygon([[p[0], p[1]] for p in shadowShape[0]])\n",
    "            return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandModule:\n",
    "    def __init__(self, entry_exit_tuples, lr=1.0, show_details=True, refresh_model=False):\n",
    "\n",
    "        print(\"Generating demand predictor...\",end=\"\")\n",
    "        \n",
    "        n_components = 2  \n",
    "        self._gm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        self._gm.fit(entry_exit_tuples[['enter_hour', 'exit_hour']])\n",
    "\n",
    "        model_path = os.path.join('_models', 'demand_cnnlstm_model.keras')\n",
    "        # Cargamos o entrenamos el modelo\n",
    "        if os.path.exists(model_path) and not refresh_model:\n",
    "            print(f\"\\n\\tLoading model from {model_path}...\", end=\"\")\n",
    "            model = load_model(model_path)\n",
    "            print(\"DONE!\")\n",
    "            self._model\n",
    "        else:\n",
    "            print(f\"\\n\\Training model...\", end=\"\")\n",
    "\n",
    "            n_incoming_veh_df= entry_exit_tuples.groupby('date').size().reset_index()\n",
    "            n_incoming_veh_df['datetime'] = pd.to_datetime(n_incoming_veh_df['date'])\n",
    "\n",
    "            # Establecer 'datetime' como índice\n",
    "            n_incoming_veh_df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Renombrar la columna 'num_vehicles'\n",
    "            n_incoming_veh_df.rename(columns={0: \"num_vehicles\"}, inplace=True)\n",
    "\n",
    "            # Eliminar columnas originales si ya no se necesitan\n",
    "            n_incoming_veh_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            n_incoming_veh_df['num_vehicles'] = scaler.fit_transform(n_incoming_veh_df[['num_vehicles']])\n",
    "\n",
    "            sequence_length = 12  # Longitud de las secuencias\n",
    "            values = n_incoming_veh_df['num_vehicles'].values\n",
    "            X, y = self._create_unidimensional_sequences(values, sequence_length)\n",
    "\n",
    "            # Dividir los datos en conjunto de entrenamiento y prueba\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-lr, random_state=42)\n",
    "\n",
    "            # Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\n",
    "            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "            X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "            # Definir el modelo CNN-LSTM\n",
    "            self._model = Sequential([\n",
    "                Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                LSTM(50, activation='relu', return_sequences=False),\n",
    "                Dense(1)\n",
    "            ])\n",
    "\n",
    "            self._model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "            _verbose= 0\n",
    "            if show_details:\n",
    "                _verbose= 1\n",
    "            # Configuración de EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "                patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "                restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "                verbose=_verbose          # Mostrar mensajes\n",
    "            )\n",
    "\n",
    "            # Entrenamiento con EarlyStopping\n",
    "            self._model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=100,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping],  # Incluir el callback\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            self._last_sequence = X_test[-1].flatten() \n",
    "            self._model.save(model_path)\n",
    "\n",
    "            print(\"DONE!\")\n",
    "\n",
    "    # Crear los datos de entrada y salida para la serie temporal\n",
    "    def _create_unidimensional_sequences(self, data, sequence_length):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - sequence_length):\n",
    "            X.append(data[i:i + sequence_length])\n",
    "            y.append(data[i + sequence_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def _predict_future_occupancy(self, n_days_ahead):\n",
    "\n",
    "        future_predictions = []\n",
    "        current_sequence = self._last_sequence.copy()\n",
    "\n",
    "        for _ in range(n_days_ahead*24):\n",
    "            # Redimensionar la secuencia actual para que sea compatible con el modelo\n",
    "            input_data = np.array(current_sequence).reshape((1, self.sequence_length, 1))\n",
    "            \n",
    "            # Predecir el siguiente valor\n",
    "            next_pred = self._model.predict(input_data, verbose=0)[0][0]\n",
    "            \n",
    "            # Guardar el valor predicho (desnormalizado)\n",
    "            future_predictions.append(self.scaler.inverse_transform([[next_pred]])[0][0])\n",
    "\n",
    "            # Actualizar la secuencia de entrada con la nueva predicción\n",
    "            current_sequence = np.append(current_sequence[1:], next_pred)\n",
    "\n",
    "        return future_predictions\n",
    "\n",
    "    def _generate_entry_and_exit_hours(self, n_tuples):\n",
    "        new_samples, _ = gm.sample(n_samples=n_tuples)\n",
    "        return [[round(x[0]),round(x[1])] for x in new_samples]\n",
    "    \n",
    "    def generate_occupancy(self, num_days_ahead):\n",
    "        \"\"\"\n",
    "        Simulates vehicle parking occupancy and generates a table of occupancy information.\n",
    "        \"\"\"\n",
    "        n_vehicles_per_hour = self._predict_future_occupancy(num_days_ahead)\n",
    "        \n",
    "        for n_vehicles in n_vehicles_per_hour: \n",
    "        \n",
    "            new_samples, _ = gm.sample(n_samples=n_vehicles)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_3haCJlsN8hF"
   },
   "outputs": [],
   "source": [
    "class ClimaticPark:\n",
    "    def __init__(self, file_name_lots='data/parking_lots.geojson',\n",
    "                 file_name_roofs='data/parking_roofs.geojson',\n",
    "                 file_name_coords='data/parking_coordinates.csv',\n",
    "                 file_name_gates='data/gates_coordinates.csv',\n",
    "                 file_name_mapcenter='data/map_center_coordinates.csv',\n",
    "                 file_name_cabintem='data/historical_cabin_temp.csv',\n",
    "                 file_name_tuples='data/entry_exit_tuples.csv'):\n",
    "        \"\"\"\n",
    "        Initializes the ClimaticPark object by loading all necessary files.\n",
    "        \"\"\"\n",
    "        # Load GeoJSON files for lots and roofs\n",
    "        self.lots_data = self._load_geojson(file_name_lots)\n",
    "        self.roofs_data = self._load_geojson(file_name_roofs)\n",
    "\n",
    "        # Add 'height' column to roofs_data with a value of 1 for all rows\n",
    "        if (self.lots_data is not None) and ('height' not in self.lots_data.columns):\n",
    "            self.lots_data['height'] = 1  # Assuming a default height of 1\n",
    "\n",
    "        # Add 'height' column to roofs_data with a value of 1 for all rows\n",
    "        if (self.roofs_data is not None) and ('height' not in self.roofs_data.columns):\n",
    "            self.roofs_data['height'] = 1  # Assuming a default height of 1\n",
    "\n",
    "        # Load CSV files for coordinates, historical data, and additional data\n",
    "        self.coords_data = self._load_csv(file_name_coords)\n",
    "        self.gates_data = self._load_csv(file_name_gates)\n",
    "      \n",
    "        self.data_no_roof =  self._load_csv(os.path.join('data', 'cabin_temperature_no_roof.csv'))\n",
    "        self.data_no_roof['coverage']=0\n",
    "        self.data_roof =  self._load_csv(os.path.join('data', 'cabin_temperature_w_roof.csv'))\n",
    "        self.data_roof['coverage']=1\n",
    "\n",
    "        self.recorded_cabin_temp = pd.read_csv(file_name_cabintem, index_col=0)\n",
    "        # Convertir la columna DateTime a tipo datetime\n",
    "        self.recorded_cabin_temp['DateTime'] = pd.to_datetime(self.recorded_cabin_temp['DateTime'])\n",
    "        # Establecer la columna DateTime como índice\n",
    "        self.recorded_cabin_temp.set_index('DateTime', inplace=True)\n",
    "        # Remuestrear los datos para obtener una frecuencia de 1 hora (calculando la media)\n",
    "        self.recorded_cabin_temp = self.recorded_cabin_temp.resample('h').mean()\n",
    "\n",
    "        self.entry_exit_tuples= pd.read_csv(os.path.join('data', 'entry_exit_tuples_clean.csv'),\n",
    "                                          index_col=0, dtype={'id_subject':str}, parse_dates=['date'])\n",
    "        \n",
    "        os.makedirs('_models', exist_ok=True)\n",
    "        self.cabin_temp_model = None\n",
    "        self.cabin_coverage_model= None\n",
    "        self.demand_module = DemandModule(self.entry_exit_tuples)\n",
    "        \n",
    "        lat = self.coords_data['latitude'].iloc[0]  \n",
    "        lon = self.coords_data['longitude'].iloc[0]\n",
    "        self.shadow_module = ShadowModule(lat, lon, self.roofs_data, self.lots_data)\n",
    "\n",
    "    def _load_geojson(self, file_name):\n",
    "        \"\"\"\n",
    "        Loads a GeoJSON file into a GeoDataFrame.\n",
    "        \"\"\"\n",
    "        if file_name:\n",
    "            return gpd.read_file(file_name)\n",
    "        else:\n",
    "            print(f\"No GeoJSON file provided for {file_name}.\")\n",
    "            return None\n",
    "\n",
    "    def _load_csv(self, file_name):\n",
    "        \"\"\"\n",
    "        Loads a CSV file into a DataFrame.\n",
    "        \"\"\"\n",
    "        if file_name:\n",
    "            return pd.read_csv(file_name)\n",
    "        else:\n",
    "            print(f\"No CSV file provided for {file_name}.\")\n",
    "            return None\n",
    "\n",
    "    def prepare_simulation(self, lr=0.8, display_details=False):\n",
    "        if not self.coords_data.empty:\n",
    "            latitude = self.coords_data['latitude'].iloc[0]  # Get the first coordinate\n",
    "            longitude = self.coords_data['longitude'].iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"The coordinates file is empty or not formatted correctly.\")\n",
    "\n",
    "        init_date =  self.recorded_cabin_temp.index[0].date()\n",
    "        final_date =  self.recorded_cabin_temp.index[-1].date()\n",
    "\n",
    "        # Process temperature data\n",
    "        print(\"Fetching historical weather conditions of the TPL...\",end=\"\")\n",
    "        ambient_temp_df = self._fetch_historical_temperature(latitude, longitude, init_date, final_date)\n",
    "        print(\"DONE!\")\n",
    "        combined_temp_df= pd.concat([ambient_temp_df, self.recorded_cabin_temp], axis=1)\n",
    "        combined_temp_df = combined_temp_df.dropna()\n",
    "\n",
    "\n",
    "        print(\"Training cabin temperature predictors...\",end=\"\")\n",
    "        self.cabin_temp_model, self.temp_scaler, self.cabin_temp_scaler = self._train_cabin_temperature_model(combined_temp_df, \n",
    "                                                                                                              lr, \n",
    "                                                                                                              display_details)\n",
    "        self.cabin_coverage_model= self._train_cabin_temperature_and_coverage_model()\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        print(\"Training demand predictor...\",end=\"\")\n",
    "        self.demand_model = self._train_demand_model(lr)\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        print(\"Training GMM of entry-exit hours...\",end=\"\")\n",
    "        self.gmm_demand= self._train_entry_exit_gmm()\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        print(\"Simulation ready to go!!\")\n",
    "\n",
    "    def launch_simulation(self, n_days_ahead, display_details=True):\n",
    "        \"\"\"\n",
    "        Lauch simulation for n_days_ahead \n",
    "        \"\"\"\n",
    "\n",
    "        if not self.coords_data.empty:\n",
    "            latitude = self.coords_data['latitude'].iloc[0]  # Get the first coordinate\n",
    "            longitude = self.coords_data['longitude'].iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"The coordinates file is empty or not formatted correctly.\")\n",
    "\n",
    "        init_day = self.entry_exit_tuples.max('date').date()\n",
    "\n",
    "        date_lst = [init_day + timedelta(days=i) for i in range(n_days_ahead)]\n",
    "        days_lst = [date.date() for date in date_lst]\n",
    "\n",
    "        ambient_temp= self._fetch_historical_temperature(self, latitude, longitude, init_day, days_lst[-1])\n",
    "        ambient_temp= ambient_temp.loc[init_date_dt:final_date_dt]\n",
    "        uncovered_cabin_temp= self._forecast_uncovered_cabin_temperatures(ambient_temp)\n",
    "        \n",
    "        coverage_rates= self.shadow_module.compute_coverage_rates(days_lst)\n",
    "\n",
    "\n",
    "    # Función para obtener datos horarios históricos de Open-Meteo API\n",
    "    def _fetch_historical_temperature(self, latitude, longitude, start_date, end_date):\n",
    "          \"\"\"\n",
    "          Obtiene datos horarios históricos de Open-Meteo API.\n",
    "\n",
    "          Args:\n",
    "          - latitude (float): Latitud de la ubicación.\n",
    "          - longitude (float): Longitud de la ubicación.\n",
    "          - start_date (str): Fecha de inicio en formato YYYY-MM-DD.\n",
    "          - end_date (str): Fecha de fin en formato YYYY-MM-DD.\n",
    "          - parameters (list): Variables meteorológicas a consultar, ej. ['temperature_2m', 'humidity_2m'].\n",
    "\n",
    "          Returns:\n",
    "          - pd.DataFrame: Datos meteorológicos horarios como DataFrame.\n",
    "          \"\"\"\n",
    "          base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "\n",
    "          # Crear el payload para la solicitud\n",
    "          payload = {\n",
    "              \"latitude\": latitude,\n",
    "              \"longitude\": longitude,\n",
    "              \"start_date\": start_date,\n",
    "              \"end_date\": end_date,\n",
    "              \"hourly\": 'temperature_2m',\n",
    "              \"timezone\": \"auto\"\n",
    "          }\n",
    "\n",
    "          # Realizar la solicitud a la API\n",
    "          response = requests.get(base_url, params=payload)\n",
    "\n",
    "          if response.status_code == 200:\n",
    "              # Convertir la respuesta JSON a un DataFrame\n",
    "              data = response.json()\n",
    "              if \"hourly\" in data:\n",
    "                  df = pd.DataFrame(data[\"hourly\"])\n",
    "                  df['time'] = pd.to_datetime(df['time'])\n",
    "                  df= df.set_index('time')\n",
    "                  return df\n",
    "              else:\n",
    "                  print(\"No se encontraron datos en la respuesta.\")\n",
    "                  return pd.DataFrame()\n",
    "          else:\n",
    "              print(f\"Error en la solicitud: {response.status_code} - {response.text}\")\n",
    "              return pd.DataFrame()\n",
    "\n",
    "    # Preparamos el dataset para secuencias\n",
    "    def _create_sequences(self, data, look_back):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - look_back):\n",
    "            X.append(data[i:i + look_back, 0])  # Secuencias de la variable 'a'\n",
    "            y.append(data[i + look_back, 1])  # Predicción futura de la variable 'b'\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "    def _train_cabin_temperature_model(self, combined_temp_df, lr=1.0, show_details=False, refresh_model=False):\n",
    "        \"\"\"\n",
    "        Trains a LSTM model using the combined temperature data.\n",
    "\n",
    "        :param combined_temp_data: DataFrame containing combined temperature data\n",
    "        \"\"\"\n",
    "        model_path = os.path.join('_models', 'cabintemp_lstm_model.keras')\n",
    "\n",
    "        # Normalización de los datos\n",
    "        scaler_a = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler_b = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "        data = combined_temp_df.copy()\n",
    "        data[\"temperature_2m\"] = scaler_a.fit_transform(combined_temp_df[\"temperature_2m\"].values.reshape(-1, 1))\n",
    "        data[\"recorded_cabin_temp\"] = scaler_b.fit_transform(combined_temp_df[\"recorded_cabin_temp\"].values.reshape(-1, 1))\n",
    "\n",
    "        look_back = 12  # Número de pasos anteriores a considerar\n",
    "        X, y = self._create_sequences(data.values, look_back)\n",
    "\n",
    "        # Dividimos en entrenamiento y prueba\n",
    "        train_size = int(len(X) * lr)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "        # Redimensionamos las entradas para LSTM [samples, time steps, features]\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        model = None\n",
    "        # Cargamos o entrenamos el modelo\n",
    "        if os.path.exists(model_path) and not refresh_model:\n",
    "            print(f\"\\n\\t Loading model from {model_path}...\")\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print(f\"\\n\\tTraining and saving model in {model_path}...\")\n",
    "\n",
    "            _verbose= 0\n",
    "            if show_details:\n",
    "              _verbose= 1\n",
    "            # Configuración de EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "                patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "                restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "                verbose=_verbose           # Mostrar mensajes\n",
    "            )\n",
    "\n",
    "\n",
    "            # Construcción del modelo LSTM\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(50, input_shape=(look_back, 1)))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "            # Entrenamiento con EarlyStopping\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=1000,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping],  # Incluir el callback\n",
    "                verbose=1\n",
    "            )\n",
    "        model.save(model_path)\n",
    "\n",
    "        return model, scaler_a, scaler_b\n",
    "\n",
    "    def _forecast_uncovered_cabin_temperatures(self, ambient_temp):\n",
    "        ambient_temp_scaled = self.temp_scaler.fit_transform(ambient_temp)\n",
    "        y_pred = self.cabin_temp_model.predict(ambient_temp_scaled)\n",
    "        y_pred_rescaled = self.cabin_temp_scaler.inverse_transform(y_pred)\n",
    "        return y_pred_rescaled\n",
    "\n",
    "    def _train_cabin_temperature_and_coverage_model(self):\n",
    "\n",
    "        temp_data = pd.concat([self.data_no_roof, self.data_roof], axis=0)\n",
    "\n",
    "        X= temp_data['T temp_ext coverage'.split()].values\n",
    "        y=  temp_data['temp_int'].values\n",
    "\n",
    "        clf = LinearRegression().fit(X, y)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "okbjHghGOYix",
    "outputId": "c4f6263c-ddf9-4dfa-d05f-59486896d7c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical weather conditions of the TPL...DONE!\n",
      "Training cabin temperature predictor...Loading model from _models\\cabintemp_lstm_model.keras...\n",
      "DONE!\n",
      "Generando modelo de temperatura de cabina y cover rates...DONE!\n",
      "Generating demand predictor...\n",
      "\tLoading model from _models\\demand_cnnlstm_model.keras...DONE!\n",
      "Training gaussian mixture model of entry-exit hours...DONE!\n",
      "Simulation ready to go!!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m park \u001b[38;5;241m=\u001b[39m ClimaticPark() \u001b[38;5;66;03m#default parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m park\u001b[38;5;241m.\u001b[39mprepare_simulation()\n\u001b[1;32m----> 3\u001b[0m park\u001b[38;5;241m.\u001b[39mlaunch_simulation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-10-24 07:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 115\u001b[0m, in \u001b[0;36mClimaticPark.launch_simulation\u001b[1;34m(self, n_days_ahead, display_details)\u001b[0m\n\u001b[0;32m    112\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe coordinates file is empty or not formatted correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Obtenemos solo la fecha (sin la hora)\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m init_day \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_exit_tuples_clean\u001b[38;5;241m.\u001b[39mmax[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdate()\n\u001b[0;32m    117\u001b[0m date_lst \u001b[38;5;241m=\u001b[39m [init_day \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_days_ahead)]\n\u001b[0;32m    118\u001b[0m days_lst \u001b[38;5;241m=\u001b[39m [date\u001b[38;5;241m.\u001b[39mdate() \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m date_lst]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "park = ClimaticPark() #default parameters\n",
    "park.prepare_simulation()\n",
    "park.launch_simulation('2022-10-24 07:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eB0DJwZqK0N"
   },
   "outputs": [],
   "source": [
    "raw_park_records_df= pd.read_csv(os.path.join('data', 'entry_exit_tuples_clean.csv'), index_col=0,\n",
    "                              dtype={'id_subject':str}, parse_dates=['date'])\n",
    "raw_park_records_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ss7TwqaHPZyM"
   },
   "outputs": [],
   "source": [
    "init_day= raw_park_records_df['date'].max()\n",
    "print(init_day)\n",
    "n_days = 4\n",
    "date_lst = [init_day + timedelta(days=i) for i in range(n_days)]\n",
    "days_lst= [date.date() for date in date_lst]\n",
    "\n",
    "print(days_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJlj2AcmdH5i"
   },
   "outputs": [],
   "source": [
    "n_components = 2  # Número de componentes de la mezcla\n",
    "gm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "gm.fit(raw_park_records_df[['enter_hour', 'exit_hour']])\n",
    "\n",
    "new_samples, _ = gm.sample(n_samples=100)\n",
    "[[round(x[0]),round(x[1])] for x in new_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b13rN6MlMOGm"
   },
   "outputs": [],
   "source": [
    "n_incoming_veh_df= raw_park_records_df.groupby('date').size().reset_index()\n",
    "# Crear columna datetime a partir de 'date' y 'enter_hour'\n",
    "n_incoming_veh_df['datetime'] = pd.to_datetime(n_incoming_veh_df['date'])\n",
    "\n",
    "# Establecer 'datetime' como índice\n",
    "n_incoming_veh_df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Renombrar la columna 'num_vehicles'\n",
    "n_incoming_veh_df.rename(columns={0: \"num_vehicles\"}, inplace=True)\n",
    "\n",
    "# Eliminar columnas originales si ya no se necesitan\n",
    "n_incoming_veh_df.drop(columns=[\"date\"], inplace=True)\n",
    "n_incoming_veh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNIvSE1PM5IM"
   },
   "outputs": [],
   "source": [
    "ax=n_incoming_veh_df.plot(figsize=(15,5), grid=True)\n",
    "ax.set_xlabel('Date',fontsize=20);\n",
    "ax.set_ylabel('Num. of hourly users',fontsize=20);\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.tick_params(axis='x', labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFXCAlxhQtOT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "n_incoming_veh_df['num_vehicles'] = scaler.fit_transform(n_incoming_veh_df[['num_vehicles']])\n",
    "\n",
    "# Crear los datos de entrada y salida para la serie temporal\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 12  # Longitud de las secuencias\n",
    "values = n_incoming_veh_df['num_vehicles'].values\n",
    "X, y = create_sequences(values, sequence_length)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Definir el modelo CNN-LSTM\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.LSTM(50, activation='relu', return_sequences=False),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Configuración de EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "    patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "    restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "    verbose=1          # Mostrar mensajes\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "#history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Entrenamiento con EarlyStopping\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],  # Incluir el callback\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Pérdida en el conjunto de prueba: {loss}\")\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Desescalar los resultados para obtener valores originales\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
    "\n",
    "print(f\"Valores originales predichos: {y_pred_rescaled[:5].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcuemtgobikt"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Función para obtener datos horarios históricos de Open-Meteo API\n",
    "def get_historical_weather_hourly(latitude, longitude, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Obtiene datos horarios históricos de Open-Meteo API.\n",
    "\n",
    "    Args:\n",
    "    - latitude (float): Latitud de la ubicación.\n",
    "    - longitude (float): Longitud de la ubicación.\n",
    "    - start_date (str): Fecha de inicio en formato YYYY-MM-DD.\n",
    "    - end_date (str): Fecha de fin en formato YYYY-MM-DD.\n",
    "    - parameters (list): Variables meteorológicas a consultar, ej. ['temperature_2m', 'humidity_2m'].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Datos meteorológicos horarios como DataFrame.\n",
    "    \"\"\"\n",
    "    base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "\n",
    "    # Crear el payload para la solicitud\n",
    "    payload = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": 'temperature_2m',\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "\n",
    "    # Realizar la solicitud a la API\n",
    "    response = requests.get(base_url, params=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Convertir la respuesta JSON a un DataFrame\n",
    "        data = response.json()\n",
    "        if \"hourly\" in data:\n",
    "            df = pd.DataFrame(data[\"hourly\"])\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No se encontraron datos en la respuesta.\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"Error en la solicitud: {response.status_code} - {response.text}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "ambient_temperatures_df = get_historical_weather_hourly(park.coords_data['latitude'].iloc[0], park.coords_data['longitude'].iloc[0], init_date, final_date)\n",
    "ambient_temperatures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utVwpNvnk-iP"
   },
   "outputs": [],
   "source": [
    "ambient_temperatures_df['time'] = pd.to_datetime(ambient_temperatures_df['time'])\n",
    "\n",
    "ambient_temperatures_df= ambient_temperatures_df.set_index('time')\n",
    "ambient_temperatures_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
