{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BooWVfJkNRfs",
    "outputId": "461d7b73-52f7-48c4-f3d0-26ca621a4459"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries (see configuration file climaticpark_env.yml in github repo)\n",
    "#!pip install pybdshadow contextily folium pillow timezonefinder plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "Gw2SxXJJNyhf"
   },
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime as dt \n",
    "from datetime import timedelta\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "import pytz\n",
    "\n",
    "import branca\n",
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, mapping\n",
    "\n",
    "from IPython.core.display import display\n",
    "from IPython.display import IFrame\n",
    "\n",
    "from suncalc import get_position\n",
    "from pyproj import CRS,Transformer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayMs = 1000 * 60 * 60 * 24\n",
    "J1970 = 2440588\n",
    "J2000 = 2451545\n",
    "def to_milliseconds(date):\n",
    "    # datetime.datetime\n",
    "    if isinstance(date, dt):\n",
    "        return date.timestamp() * 1000\n",
    "\n",
    "    # Pandas series of Pandas datetime objects\n",
    "    if pd and pd.api.types.is_datetime64_any_dtype(date):\n",
    "        # A datetime-like series coerce to int is (always?) in nanoseconds\n",
    "        return date.astype('int64') / 10 ** 6\n",
    "\n",
    "    # Single pandas Timestamp\n",
    "    if pd and isinstance(date, pd.Timestamp):\n",
    "        date = date.to_numpy()\n",
    "\n",
    "    # Numpy datetime64\n",
    "    if np.issubdtype(date.dtype, np.datetime64):\n",
    "        return date.astype('datetime64[ms]').astype('int64')\n",
    "\n",
    "    # Last-ditch effort\n",
    "    if pd:\n",
    "        return np.array(pd.to_datetime(date).astype('int64') / 10 ** 6)\n",
    "\n",
    "    raise ValueError(f'Unknown date type: {type(date)}')\n",
    "\n",
    "\n",
    "def to_julian(date):\n",
    "    return to_milliseconds(date) / dayMs - 0.5 + J1970\n",
    "\n",
    "\n",
    "def from_julian(j):\n",
    "    ms_date = (j + 0.5 - J1970) * dayMs\n",
    "\n",
    "    if pd:\n",
    "        # If a single value, coerce to a pd.Timestamp\n",
    "        if np.prod(np.array(ms_date).shape) == 1:\n",
    "            return pd.to_datetime(ms_date, unit='ms')\n",
    "\n",
    "        # .astype(datetime) is much faster than pd.to_datetime but it only works\n",
    "        # on series of dates, not on a single pd.Timestamp, so I fall back to\n",
    "        # pd.to_datetime for that.\n",
    "        try:\n",
    "            return (pd.Series(ms_date) * 1e6).astype('datetime64[ns, UTC]')\n",
    "        except TypeError:\n",
    "            return pd.to_datetime(ms_date, unit='ms')\n",
    "\n",
    "    # ms_date could be iterable\n",
    "    try:\n",
    "        return np.array([\n",
    "            datetime.utcfromtimestamp(x / 1000)\n",
    "            if not np.isnan(x) else np.datetime64('NaT') for x in ms_date])\n",
    "\n",
    "    except TypeError:\n",
    "        return datetime.utcfromtimestamp(\n",
    "            ms_date / 1000) if not np.isnan(ms_date) else np.datetime64('NaT')\n",
    "\n",
    "\n",
    "def to_days(date):\n",
    "    return to_julian(date) - J2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShadowModule:\n",
    "        \n",
    "    def __init__(self,lat, lon, roofs, spaces):\n",
    "        self.lat= lat\n",
    "        self.lon= lon\n",
    "        self.roofs = roofs\n",
    "        self.spaces= spaces\n",
    "    \n",
    "    \n",
    "    def compute_coverage_rates(self, days_lst):\n",
    "        \"\"\"\n",
    "        Compute coverage rates for each day in days_lst\n",
    "        \"\"\"\n",
    "\n",
    "        # DataFrames to store results\n",
    "        #coverage_rates_df = pd.DataFrame()\n",
    "\n",
    "        #print(self.spaces.head())\n",
    "        #print(self.spaces.info())\n",
    "        #print(self.spaces.geometry)\n",
    "        #print(self.spaces['geometry'].apply(type).unique())  # Asegúrate de que todas sean geometrías válidas\n",
    "\n",
    "        coverage_rates_gdf = gpd.GeoDataFrame(geometry=self.spaces.geometry)\n",
    "        coverage_rates_gdf = coverage_rates_gdf.set_crs(epsg=4326)\n",
    "        #coverage_rates_gdf = coverage_rates_gdf.to_crs(epsg=self.spaces.crs.to_epsg())\n",
    "        \n",
    "        local_tz = pytz.timezone('Europe/Madrid')\n",
    "\n",
    "        for current_date in days_lst:\n",
    "            # Compute shadow projections for roofs\n",
    "            for hour in range(6,22):\n",
    "                \n",
    "                date_hour = dt.combine(current_date, datetime.time(hour, 0)) \n",
    "                date_hour = local_tz.localize(date_hour)  # Localizarlo en la zona horaria de Madrid\n",
    "\n",
    "                print(date_hour, type(date_hour))\n",
    "                #try:\n",
    "                shadows_gdf = self._all_sunshadeshadow_sunlight(date_hour)\n",
    "                \n",
    "                print(shadows_gdf.head())\n",
    "                print(self.roofs.head())\n",
    "\n",
    "                # Calculate coverage rates\n",
    "                intersection = gpd.overlay(coverage_rates_gdf, shadows_gdf, how='intersection')\n",
    "\n",
    "                \"\"\"\n",
    "                print(\"*\"*8,\"shadows_gdf\",\"*\"*8)\n",
    "                print(shadows_gdf.info())\n",
    "                print(shadows_gdf.head(2))                \n",
    "                print(\"*\"*8,\"intersection\",\"*\"*8)\n",
    "                print(intersection.info())\n",
    "                print(intersection.head(2))\n",
    "                print(\"*\"*8,\"coverage_rates_gdf\",\"*\"*8,)\n",
    "                print(coverage_rates_gdf.info())\n",
    "                print(coverage_rates_gdf.head(2))                \n",
    "                print(\"*\"*8)\n",
    "                \"\"\"\n",
    "\n",
    "                coverage_rates= []\n",
    "                for index, parking_space in coverage_rates_gdf.iterrows():\n",
    "                    #print(parking_space)\n",
    "                    space_total_area = parking_space.geometry.area\n",
    "                    space_shadow_area = intersection.loc[index, \"geometry\"].area if index in intersection.index else 0 #intersection.loc[index,\"geometry\"].area\n",
    "                    space_coverage= space_shadow_area / space_total_area\n",
    "                    #print(index, space_total_area,space_shadow_area,space_coverage)\n",
    "                    #print(\"*\"*8)\n",
    "                    coverage_rates.append(space_coverage)\n",
    "\n",
    "                #intersection_area = intersection.geometry.area.sum()\n",
    "                #parking_space_area = coverage_rates_gdf.geometry.area.sum()\n",
    "\n",
    "                #coverage_rates = intersection_area / parking_space_area\n",
    "                #print(coverage_rates)\n",
    "\n",
    "                \"\"\"\n",
    "                coverage_rates = []\n",
    "                for index, parking_space in self.spaces.iterrows():\n",
    "                    parking_space_gdf = gpd.GeoDataFrame(geometry=self..geometry)\n",
    "                    parking_space_gdf = parking_space_gdf.set_crs(epsg=4326)\n",
    "                    parking_space_gdf = parking_space_gdf.to_crs(epsg=shadows_gdf.crs.to_epsg())\n",
    "\n",
    "                    intersection = gpd.overlay(parking_space_gdf, shadows_gdf, how='intersection')\n",
    "\n",
    "                    intersection_area = intersection.geometry.area.sum()\n",
    "                    parking_space_area = parking_space_gdf.geometry.area.sum()\n",
    "\n",
    "                    coverage_rate = intersection_area / parking_space_area\n",
    "                    coverage_rates.append(coverage_rate)\n",
    "                \"\"\"\n",
    "\n",
    "                coverage_rates_gdf[f'coverage_rate_{date_hour.strftime(\"%Y-%m-%d %H:%M\")}'] = coverage_rates\n",
    "                #except Exception as e:\n",
    "                #    print(f\"ERROR:: ShadowModule compute_coverage_rates: {e}\")\n",
    "        self._coverage_rates = coverage_rates_gdf\n",
    "        print(self._coverage_rates)\n",
    "        coverage_rates_gdf.to_file(\"test.geojson\", driver=\"GeoJSON\")\n",
    "        return coverage_rates_gdf\n",
    "\n",
    "    def show_shadows(self):\n",
    "        # Extraer las columnas de tiempo (asegurarse de que están ordenadas)\n",
    "        coverage_cols = [col for col in self._coverage_rates.columns if \"coverage_rate_\" in col]\n",
    "        coverage_cols.sort()\n",
    "\n",
    "        # Crear el mapa base centrado en el área de los polígonos\n",
    "        m = folium.Map(\n",
    "            location=[self._coverage_rates.geometry.centroid.y.mean(), self._coverage_rates.geometry.centroid.x.mean()],\n",
    "            zoom_start=14,\n",
    "            tiles=\"cartodb positron\"\n",
    "        )\n",
    "\n",
    "        # Crear una paleta de colores basada en los valores de cobertura\n",
    "        colormap = branca.colormap.linear.YlOrRd_09.scale(\n",
    "            self._coverage_rates[coverage_cols].min().min(), self._coverage_rates[coverage_cols].max().max()\n",
    "        )\n",
    "        colormap.caption = \"Coverage Rate\"\n",
    "        colormap.add_to(m)\n",
    "\n",
    "        # Crear una estructura para `TimestampedGeoJson`\n",
    "        features = []\n",
    "\n",
    "        for index, row in self._coverage_rates.iterrows():\n",
    "            for time_index, col in enumerate(coverage_cols):\n",
    "                feature = {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": mapping(row.geometry),  # Convertir a JSON\n",
    "                    \"properties\": {\n",
    "                        \"time\": col.replace(\"coverage_rate_\", \"\"),  # Extraer la fecha y hora\n",
    "                        \"style\": {\n",
    "                            \"fillColor\": colormap(row[col]),  # Color según la cobertura\n",
    "                            \"color\": \"black\",\n",
    "                            \"weight\": 0.5,\n",
    "                            \"fillOpacity\": 0.7,\n",
    "                        },\n",
    "                        \"popup\": f\"Coverage: {row[col]:.2f}\"\n",
    "                    }\n",
    "                }\n",
    "                features.append(feature)\n",
    "\n",
    "        # Crear un `TimestampedGeoJson`\n",
    "        TimestampedGeoJson(\n",
    "            {\n",
    "                \"type\": \"FeatureCollection\",\n",
    "                \"features\": features,\n",
    "            },\n",
    "            period=\"PT1H\",  # Intervalo de tiempo (1 hora)\n",
    "            add_last_point=False,\n",
    "            auto_play=True,\n",
    "            loop=True,\n",
    "            max_speed=1,\n",
    "            loop_button=True,\n",
    "            date_options=\"YYYY-MM-DD HH:mm\",\n",
    "        ).add_to(m)\n",
    "\n",
    "        # Guardar el mapa como HTML\n",
    "        m.save(\".shadows_map.html\")\n",
    "\n",
    "        display(IFrame(\".shadows_map.html\", width=800, height=500))\n",
    "\n",
    "\n",
    "        # Define function to calculate shadow and sunlight for all rooftops\n",
    "    def _all_sunshadeshadow_sunlight(self, date):\n",
    "        roof_projected_df= self.roofs.copy()\n",
    "        #roof_projected_df['geometry'] = roof_projected_df.apply(lambda r: self._sunshadeshadow_sunlight(np.datetime64(date), r[0]), axis=1)\n",
    "        roof_projected_df['geometry'] = roof_projected_df.apply(lambda r: self._sunshadeshadow_sunlight(date.strftime('%Y-%m-%d %H:%M:%S %z'), r[0]), axis=1)\n",
    "\n",
    "        return roof_projected_df\n",
    "\n",
    "\n",
    "    def _sunshadeshadow_sunlight(self, date, r, sunshade_height=2):\n",
    "        meanlon= r.centroid.y\n",
    "        meanlat= r.centroid.x\n",
    "        # obtain sun position\n",
    "        sunPosition = get_position(date, meanlon, meanlat)\n",
    "        print(\"sun position\", date, to_days(date), sunPosition)\n",
    "        if sunPosition['altitude'] < 0:\n",
    "            raise ValueError(\"Given time before sunrise or after sunset\")\n",
    "            \n",
    "        r_coords= np.array(r.exterior.coords)\n",
    "        r_coords= r_coords.reshape(1,-1,2)\n",
    "        shape = ShadowModule.lonlat2aeqd(r_coords,meanlon,meanlat)\n",
    "        azimuth = sunPosition['azimuth']\n",
    "        altitude = sunPosition['altitude']\n",
    "\n",
    "        n = np.shape(shape)[0]\n",
    "        distance = sunshade_height / math.tan(altitude)\n",
    "\n",
    "        # calculate the offset of the projection position\n",
    "        lonDistance = distance * math.sin(azimuth)\n",
    "        latDistance = distance * math.cos(azimuth)\n",
    "\n",
    "        shadowShape = np.zeros((1, 5, 2))\n",
    "        shadowShape[:, :, :] += shape\n",
    "        shadowShape[:, :, 0] = shape[:, :, 0] + lonDistance\n",
    "        shadowShape[:, :, 1] = shape[:, :, 1] + latDistance\n",
    "        shadowShape = ShadowModule.aeqd2lonlat(shadowShape,meanlon,meanlat)\n",
    "        p = Polygon([[p[0], p[1]] for p in shadowShape[0]])\n",
    "        return p\n",
    "    \n",
    "    @staticmethod\n",
    "    def lonlat2aeqd(lonlat, center_lon, center_lat):\n",
    "        epsg = CRS.from_proj4(\"+proj=aeqd +lat_0=\"+str(center_lat) +\n",
    "                            \" +lon_0=\"+str(center_lon)+\" +datum=WGS84\")\n",
    "        transformer = Transformer.from_crs(\"EPSG:4326\", epsg, always_xy=True)\n",
    "        proj_coords = transformer.transform(lonlat[:, :, 0], lonlat[:, :, 1])\n",
    "        proj_coords = np.array(proj_coords).transpose([1, 2, 0])\n",
    "        return proj_coords\n",
    "    \n",
    "    @staticmethod\n",
    "    def aeqd2lonlat(proj_coords,meanlon,meanlat):\n",
    "        epsg = CRS.from_proj4(\"+proj=aeqd +lat_0=\"+str(meanlat)+\" +lon_0=\"+str(meanlon)+\" +datum=WGS84\")\n",
    "        transformer = Transformer.from_crs( epsg,\"EPSG:4326\",always_xy = True)\n",
    "        lonlat = transformer.transform(proj_coords[:,:,0], proj_coords[:,:,1])\n",
    "        lonlat = np.array(lonlat).transpose([1,2,0])\n",
    "        return lonlat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandModule:\n",
    "    def __init__(self, entry_exit_tuples):        \n",
    "        self._entry_exit_tuples= entry_exit_tuples\n",
    "\n",
    "    def train_demand_predictors(self, lr=0.9, sequence_length= 12, show_details=True, refresh_model=False):\n",
    "        self._sequence_length= sequence_length\n",
    "\n",
    "        #self._gm = GaussianMixture(n_components=self._n_components, random_state=42)\n",
    "        #self._gm.fit(self._entry_exit_tuples[['enter_hour', 'exit_hour']])\n",
    "\n",
    "        self._gmm_models={}\n",
    "        for hour, group in self._entry_exit_tuples.groupby('enter_hour'):\n",
    "            X = group[\"exit_hour\"].values  # Extraer los datos de las características para este grupo\n",
    "            if X.shape[0]== 1:\n",
    "                X= X.reshape(1, -1)\n",
    "            else:\n",
    "                X= X.reshape(-1,1)\n",
    "            if X.shape[0]>=2:\n",
    "                # Crear y ajustar el modelo GMM; ajusta n_components según tus necesidades\n",
    "                gmm = GaussianMixture(n_components=1, random_state=42)\n",
    "                gmm.fit(X)\n",
    "                self._gmm_models[hour] = gmm\n",
    "\n",
    "        model_path = os.path.join('_models', 'demand_cnnlstm_model.keras')\n",
    "        # Cargamos o entrenamos el modelo\n",
    "\n",
    "        n_incoming_veh_df= self._entry_exit_tuples.groupby('date').size().reset_index()\n",
    "        n_incoming_veh_df['datetime'] = pd.to_datetime(n_incoming_veh_df['date'])\n",
    "\n",
    "        # Establecer 'datetime' como índice\n",
    "        n_incoming_veh_df.set_index('datetime', inplace=True)\n",
    "\n",
    "        # Renombrar la columna 'num_vehicles'\n",
    "        n_incoming_veh_df.rename(columns={0: \"num_vehicles\"}, inplace=True)\n",
    "\n",
    "        # Eliminar columnas originales si ya no se necesitan\n",
    "        n_incoming_veh_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "        self._scaler = MinMaxScaler()\n",
    "        n_incoming_veh_df['num_vehicles'] = self._scaler.fit_transform(n_incoming_veh_df[['num_vehicles']])\n",
    "\n",
    "        values = n_incoming_veh_df['num_vehicles'].values\n",
    "        X, y = self._create_unidimensional_sequences(values, self._sequence_length)\n",
    "\n",
    "        # Dividir los datos en conjunto de entrenamiento y prueba\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-lr, random_state=42)\n",
    "\n",
    "        # Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        self._last_sequence = X_test[-1].flatten() \n",
    "\n",
    "        if os.path.exists(model_path) and not refresh_model:\n",
    "            print(f\"\\n\\tLoading demand predictor from {model_path}...\", end=\"\")\n",
    "            self._model = load_model(model_path)\n",
    "        else: \n",
    "            print(f\"\\n\\tTraining demand predictor...\", end=\"\")\n",
    "\n",
    "            # Definir el modelo CNN-LSTM\n",
    "            self._model = Sequential([\n",
    "                Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                LSTM(50, activation='relu', return_sequences=False),\n",
    "                Dense(1)\n",
    "            ])\n",
    "\n",
    "            self._model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "            _verbose= 0\n",
    "            if show_details:\n",
    "                _verbose= 1\n",
    "            # Configuración de EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "                patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "                restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "                verbose=_verbose          # Mostrar mensajes\n",
    "            )\n",
    "\n",
    "            # Entrenamiento con EarlyStopping\n",
    "            self._model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=1000,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping],  # Incluir el callback\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            self._model.save(model_path)\n",
    "\n",
    "        print(\"DONE!\")\n",
    "\n",
    "    # Crear los datos de entrada y salida para la serie temporal\n",
    "    def _create_unidimensional_sequences(self, data, sequence_length):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - sequence_length):\n",
    "            X.append(data[i:i + sequence_length])\n",
    "            y.append(data[i + sequence_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def _predict_occupancy(self, n_days_ahead):\n",
    "\n",
    "        future_predictions = []\n",
    "        current_sequence = self._last_sequence.copy()\n",
    "\n",
    "        for _ in range(n_days_ahead*24):\n",
    "            # Redimensionar la secuencia actual para que sea compatible con el modelo\n",
    "            input_data = np.array(current_sequence).reshape((1, self._sequence_length, 1))\n",
    "            \n",
    "            # Predecir el siguiente valor\n",
    "            next_pred = self._model.predict(input_data, verbose=0)[0][0]\n",
    "            \n",
    "            # Guardar el valor predicho (desnormalizado)\n",
    "            future_predictions.append(self._scaler.inverse_transform([[next_pred]])[0][0])\n",
    "\n",
    "            # Actualizar la secuencia de entrada con la nueva predicción\n",
    "            current_sequence = np.append(current_sequence[1:], next_pred)\n",
    "\n",
    "        return future_predictions\n",
    "\n",
    "    def _generate_entry_and_exit_hours(self, n_tuples):\n",
    "        new_samples, _ = gm.sample(n_samples=n_tuples)\n",
    "        return [[round(x[0]),round(x[1])] for x in new_samples]\n",
    "    \n",
    "    def generate_occupancy(self, num_days_ahead):\n",
    "        \"\"\"\n",
    "        Simulates vehicle parking occupancy and generates a table of occupancy information.\n",
    "        \"\"\"\n",
    "\n",
    "        n_vehicles_per_hour = self._predict_occupancy(num_days_ahead)\n",
    "        simulated_occupancy=[]\n",
    "        hour= 0\n",
    "        for n_vehicles in n_vehicles_per_hour: \n",
    "            if hour in self._gmm_models:\n",
    "                gm = self._gmm_models[hour]      \n",
    "                new_samples, _ = gm.sample(n_samples=n_vehicles)\n",
    "                simulated_occupancy.append([(hour, round(x[0])) for x in new_samples])\n",
    "            hour = (hour+1) % 24\n",
    "        return simulated_occupancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmbientModule:\n",
    "    \n",
    "    def __init__(self, lat, lon):\n",
    "        self.lat= lat\n",
    "        self.lon= lon\n",
    "\n",
    "    def train_ambient_temperature_model(self, start_date, end_date, lr=0.9, show_details=False, refresh_model=False):\n",
    "        \"\"\"\n",
    "        Trains a LSTM model using the combined temperature data.\n",
    "\n",
    "        :param combined_temp_data: DataFrame containing combined temperature data\n",
    "        \"\"\"\n",
    "\n",
    "        ambient_temperaure_df= self._fetch_historical_temperature(start_date, end_date)\n",
    "        model_path = os.path.join('_models', 'cabintemp_lstm_model.keras')\n",
    "\n",
    "        # Normalización de los datos\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        #scaler_b = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "        data = ambient_temperaure_df.copy()\n",
    "        data[\"temperature_2m\"] = scaler.fit_transform(ambient_temperaure_df[\"temperature_2m\"].values.reshape(-1, 1))\n",
    "\n",
    "        look_back = 12  # Número de pasos anteriores a considerar\n",
    "        X, y = ClimaticPark.create_sequences(data.values, look_back)\n",
    "\n",
    "        # Dividimos en entrenamiento y prueba\n",
    "        train_size = int(len(X) * lr)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "        # Redimensionamos las entradas para LSTM [samples, time steps, features]\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        self._last_sequence = X_test[-1].flatten() \n",
    "        \n",
    "        model = None\n",
    "        # Cargamos o entrenamos el modelo\n",
    "        if os.path.exists(model_path) and not refresh_model:\n",
    "            print(f\"\\n\\tLoading ambient temperature predictor from {model_path}...\", end=\"\")\n",
    "            model = load_model(model_path)\n",
    "            print(\"DONE!\")\n",
    "        else:\n",
    "            print(f\"\\n\\tTraining and saving model in {model_path}...\")\n",
    "\n",
    "            _verbose= 0\n",
    "            if show_details:\n",
    "              _verbose= 1\n",
    "            # Configuración de EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "                patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "                restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "                verbose=_verbose           # Mostrar mensajes\n",
    "            )\n",
    "\n",
    "\n",
    "            # Construcción del modelo LSTM\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(50, input_shape=(look_back, 1)))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "            # Entrenamiento con EarlyStopping\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=1000,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping],  # Incluir el callback\n",
    "                verbose=1\n",
    "            )\n",
    "            self._last_sequence = X_test[-1].flatten() \n",
    "\n",
    "        model.save(model_path)\n",
    "\n",
    "        self._model= model\n",
    "        self._scaler= scaler\n",
    "\n",
    "    def predict_ambient_temperature(self, n_days_ahead):\n",
    "\n",
    "        future_predictions = []\n",
    "        current_sequence = self._last_sequence.copy()\n",
    "\n",
    "        for _ in range(n_days_ahead*24):\n",
    "            # Redimensionar la secuencia actual para que sea compatible con el modelo\n",
    "            input_data = np.array(current_sequence).reshape((1, len(current_sequence), 1))\n",
    "            \n",
    "            # Predecir el siguiente valor\n",
    "            next_pred = self._model.predict(input_data, verbose=0)[0][0]\n",
    "            \n",
    "            # Guardar el valor predicho (desnormalizado)\n",
    "            future_predictions.append(self._scaler.inverse_transform([[next_pred]])[0][0])\n",
    "\n",
    "            # Actualizar la secuencia de entrada con la nueva predicción\n",
    "            current_sequence = np.append(current_sequence[1:], next_pred)\n",
    "\n",
    "        return future_predictions\n",
    "\n",
    "    # Función para obtener datos horarios históricos de Open-Meteo API\n",
    "    def _fetch_historical_temperature(self, start_date, end_date):\n",
    "          \"\"\"\n",
    "          Obtiene datos horarios históricos de Open-Meteo API.\n",
    "\n",
    "          Args:\n",
    "          - latitude (float): Latitud de la ubicación.\n",
    "          - longitude (float): Longitud de la ubicación.\n",
    "          - start_date (str): Fecha de inicio en formato YYYY-MM-DD.\n",
    "          - end_date (str): Fecha de fin en formato YYYY-MM-DD.\n",
    "          - parameters (list): Variables meteorológicas a consultar, ej. ['temperature_2m', 'humidity_2m'].\n",
    "\n",
    "          Returns:\n",
    "          - pd.DataFrame: Datos meteorológicos horarios como DataFrame.\n",
    "          \"\"\"\n",
    "          base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "\n",
    "          # Crear el payload para la solicitud\n",
    "          payload = {\n",
    "              \"latitude\": self.lat,\n",
    "              \"longitude\": self.lon,\n",
    "              \"start_date\": start_date,\n",
    "              \"end_date\": end_date,\n",
    "              \"hourly\": 'temperature_2m',\n",
    "              \"timezone\": \"auto\"\n",
    "          }\n",
    "\n",
    "          # Realizar la solicitud a la API\n",
    "          response = requests.get(base_url, params=payload)\n",
    "\n",
    "          if response.status_code == 200:\n",
    "              # Convertir la respuesta JSON a un DataFrame\n",
    "              data = response.json()\n",
    "              if \"hourly\" in data:\n",
    "                  df = pd.DataFrame(data[\"hourly\"])\n",
    "                  df['time'] = pd.to_datetime(df['time'])\n",
    "                  df= df.set_index('time')\n",
    "                  return df\n",
    "              else:\n",
    "                  print(\"No se encontraron datos en la respuesta.\")\n",
    "                  return pd.DataFrame()\n",
    "          else:\n",
    "              print(f\"Error en la solicitud: {response.status_code} - {response.text}\")\n",
    "              return pd.DataFrame()\n",
    "          \n",
    "\n",
    "    def _forecast_uncovered_cabin_temperatures(self, ambient_temp):\n",
    "        ambient_temp_scaled = self.temp_scaler.fit_transform(ambient_temp)\n",
    "        y_pred = self.cabin_temp_model.predict(ambient_temp_scaled)\n",
    "        y_pred_rescaled = self.cabin_temp_scaler.inverse_transform(y_pred)\n",
    "        return y_pred_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimaticParkState(Enum):\n",
    "    INIT = 1\n",
    "    READY = 2\n",
    "    LAUNCHED = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "_3haCJlsN8hF"
   },
   "outputs": [],
   "source": [
    "class ClimaticPark:\n",
    "    def __init__(self, file_name_lots='data/parking_lots.geojson',\n",
    "                 file_name_roofs='data/parking_roofs.geojson',\n",
    "                 file_name_coords='data/parking_coordinates.csv',\n",
    "                 file_name_gates='data/gates_coordinates.csv',\n",
    "                 file_name_mapcenter='data/map_center_coordinates.csv',\n",
    "                 file_name_cabintem='data/historical_cabin_temp.csv',\n",
    "                 file_name_tuples='data/entry_exit_tuples.csv'):\n",
    "        \"\"\"\n",
    "        Initializes the ClimaticPark object by loading all necessary files.\n",
    "        \"\"\"\n",
    "        # Load GeoJSON files for lots and roofs\n",
    "        self.lots_data = ClimaticPark.load_geojson(file_name_lots)\n",
    "        self.roofs_data = ClimaticPark.load_geojson(file_name_roofs)\n",
    "\n",
    "        # Add 'height' column to roofs_data with a value of 1 for all rows\n",
    "        if (self.lots_data is not None) and ('height' not in self.lots_data.columns):\n",
    "            self.lots_data['height'] = 0 # Assuming a default height of 1\n",
    "\n",
    "        # Add 'height' column to roofs_data with a value of 1 for all rows\n",
    "        if (self.roofs_data is not None) and ('height' not in self.roofs_data.columns):\n",
    "            self.roofs_data['height'] = 2  # Assuming a default height of 1\n",
    "\n",
    "        # Load CSV files for coordinates, historical data, and additional data\n",
    "        self.coords_data = ClimaticPark.load_csv(file_name_coords)\n",
    "        self.gates_data = ClimaticPark.load_csv(file_name_gates)\n",
    "      \n",
    "        self.data_no_roof =  ClimaticPark.load_csv(os.path.join('data', 'cabin_temperature_no_roof.csv'))\n",
    "        self.data_no_roof['coverage']=0\n",
    "        self.data_roof =  ClimaticPark.load_csv(os.path.join('data', 'cabin_temperature_w_roof.csv'))\n",
    "        self.data_roof['coverage']=1\n",
    "\n",
    "        self.recorded_cabin_temp = pd.read_csv(file_name_cabintem, index_col=0)\n",
    "        # Convertir la columna DateTime a tipo datetime\n",
    "        self.recorded_cabin_temp['DateTime'] = pd.to_datetime(self.recorded_cabin_temp['DateTime'])\n",
    "        # Establecer la columna DateTime como índice\n",
    "        self.recorded_cabin_temp.set_index('DateTime', inplace=True)\n",
    "        # Remuestrear los datos para obtener una frecuencia de 1 hora (calculando la media)\n",
    "        self.recorded_cabin_temp = self.recorded_cabin_temp.resample('h').mean()\n",
    "\n",
    "        self.entry_exit_tuples= pd.read_csv(os.path.join('data', 'entry_exit_tuples_clean.csv'), index_col=0, dtype={'id_subject':str}, parse_dates=['date'])\n",
    "        \n",
    "        os.makedirs('_models', exist_ok=True)\n",
    "        self.cabin_temp_model = None\n",
    "        self.cabin_coverage_model= None\n",
    "        print(\"Generating Demand Module...\", end=\"\")\n",
    "        self.demand_module = DemandModule(self.entry_exit_tuples)\n",
    "        print(\"DONE!\")\n",
    "        \n",
    "        lat = self.coords_data['latitude'].iloc[0]  \n",
    "        lon = self.coords_data['longitude'].iloc[0]\n",
    "        \n",
    "        print(\"Generating Shadow Module...\", end=\"\")\n",
    "        self.shadow_module = ShadowModule(lat, lon, self.roofs_data, self.lots_data)\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        print(\"Generating Ambient Module...\", end=\"\")\n",
    "        #self.ambient_module = AmbientModule(lat,lon)\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        self._state = ClimaticParkState.INIT\n",
    "\n",
    "    @staticmethod\n",
    "    def load_geojson(file_name):\n",
    "        \"\"\"\n",
    "        Loads a GeoJSON file into a GeoDataFrame.\n",
    "        \"\"\"\n",
    "        if file_name:\n",
    "            return gpd.read_file(file_name).set_geometry(\"geometry\")\n",
    "        else:\n",
    "            print(f\"No GeoJSON file provided for {file_name}.\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_csv(file_name):\n",
    "        \"\"\"\n",
    "        Loads a CSV file into a DataFrame.\n",
    "        \"\"\"\n",
    "        if file_name:\n",
    "            return pd.read_csv(file_name)\n",
    "        else:\n",
    "            print(f\"No CSV file provided for {file_name}.\")\n",
    "            return None\n",
    "\n",
    "    def prepare_simulation(self, lr=0.8, display_details=False):\n",
    "        print(\"Preparing simulation for TPL...\")\n",
    "\n",
    "        init_date =  self.recorded_cabin_temp.index[0].date()\n",
    "        final_date =  self.recorded_cabin_temp.index[-1].date()\n",
    "\n",
    "        # Process temperature data\n",
    "        print(\"\\tTraining ambient temperature predictors...\",end=\"\")\n",
    "        #self.ambient_module.train_ambient_temperature_model(init_date, final_date)\n",
    "        #combined_temp_df= pd.concat([ambient_temp_df, self.recorded_cabin_temp], axis=1)\n",
    "        #combined_temp_df = combined_temp_df.dropna()\n",
    "\n",
    "\n",
    "        #print(\"Training cabin temperature predictors...\",end=\"\")\n",
    "        #self.cabin_temp_model, self.temp_scaler, self.cabin_temp_scaler = self._train_cabin_temperature_model(combined_temp_df, lr, display_details)\n",
    "        #self.cabin_coverage_model= self._train_cabin_temperature_and_coverage_model()\n",
    "        #print(\"DONE!\")\n",
    "\n",
    "        print(\"\\tTraining demand predictors...\",end=\"\")\n",
    "        self.demand_module.train_demand_predictors()  \n",
    "\n",
    "        self._state = ClimaticParkState.READY\n",
    "\n",
    "        print(\"Simulation ready to go!!\")\n",
    "\n",
    "    def launch_simulation(self, n_days_ahead, display_details=True):\n",
    "        \"\"\"\n",
    "        Lauch simulation for n_days_ahead \n",
    "        \"\"\"\n",
    "        print(\"Starting simulation of TPL...\")\n",
    "\n",
    "        self.demand_module.generate_occupancy(n_days_ahead)\n",
    "        \n",
    "        init_day = self.entry_exit_tuples['date'].max().date()\n",
    "        print(type(init_day))\n",
    "        \n",
    "        madrid_tz = pytz.timezone(\"Europe/Madrid\")\n",
    "\n",
    "        init_day = init_day.tz_localize('UTC').tz_convert(madrid_tz)\n",
    "\n",
    "        date_lst = [init_day + timedelta(days=i) for i in range(n_days_ahead)]\n",
    "        print(date_lst)\n",
    "\n",
    "        simulated_occupancy = self.demand_module.generate_occupancy(n_days_ahead)    \n",
    "\n",
    "        simulated_shadows= self.shadow_module.compute_coverage_rates(date_lst)\n",
    "        \n",
    "        self._state = ClimaticParkState.LAUNCHED\n",
    "\n",
    "    def show_shadows_map(self):\n",
    "        if self._state == ClimaticParkState.LAUNCHED:\n",
    "            self.shadow_module.show_shadows()\n",
    "        else:\n",
    "            print(\"You must first launch the simulation by calling the launch_simulation method.\")\n",
    "\n",
    "    # Preparamos el dataset para secuencias\n",
    "    @staticmethod\n",
    "    def create_sequences(data, look_back):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - look_back):\n",
    "            X.append(data[i:i + look_back])  # Secuencias de la variable 'a'\n",
    "            y.append(data[i + look_back])  # Predicción futura de la variable 'b'\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def _train_cabin_temperature_and_coverage_model(self):\n",
    "\n",
    "        temp_data = pd.concat([self.data_no_roof, self.data_roof], axis=0)\n",
    "\n",
    "        X= temp_data['T temp_ext coverage'.split()].values\n",
    "        y=  temp_data['temp_int'].values\n",
    "\n",
    "        clf = LinearRegression().fit(X, y)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "okbjHghGOYix",
    "outputId": "c4f6263c-ddf9-4dfa-d05f-59486896d7c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Demand Module...DONE!\n",
      "Generating Shadow Module...DONE!\n",
      "Generating Ambient Module...DONE!\n",
      "Preparing simulation for TPL...\n",
      "\tTraining ambient temperature predictors...\tTraining demand predictors...\n",
      "\tLoading demand predictor from _models\\demand_cnnlstm_model.keras...DONE!\n",
      "Simulation ready to go!!\n"
     ]
    }
   ],
   "source": [
    "park = ClimaticPark() # default parameters\n",
    "park.prepare_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation of TPL...\n",
      "<class 'datetime.date'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'datetime.date' object has no attribute 'tz_localize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[165], line 123\u001b[0m, in \u001b[0;36mClimaticPark.launch_simulation\u001b[1;34m(self, n_days_ahead, display_details)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(init_day))\n\u001b[0;32m    121\u001b[0m madrid_tz \u001b[38;5;241m=\u001b[39m pytz\u001b[38;5;241m.\u001b[39mtimezone(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEurope/Madrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m init_day \u001b[38;5;241m=\u001b[39m \u001b[43minit_day\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz_localize\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtz_convert(madrid_tz)\n\u001b[0;32m    125\u001b[0m date_lst \u001b[38;5;241m=\u001b[39m [init_day \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_days_ahead)]\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(date_lst)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'datetime.date' object has no attribute 'tz_localize'"
     ]
    }
   ],
   "source": [
    "park.launch_simulation(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\".shadows_map.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1621cfb0610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "park.show_shadows_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFXCAlxhQtOT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import train_test_split\\n\\nscaler = MinMaxScaler()\\nn_incoming_veh_df[\\'num_vehicles\\'] = scaler.fit_transform(n_incoming_veh_df[[\\'num_vehicles\\']])\\n\\n# Crear los datos de entrada y salida para la serie temporal\\ndef create_sequences(data, sequence_length):\\n    X, y = [], []\\n    for i in range(len(data) - sequence_length):\\n        X.append(data[i:i + sequence_length])\\n        y.append(data[i + sequence_length])\\n    return np.array(X), np.array(y)\\n\\nsequence_length = 12  # Longitud de las secuencias\\nvalues = n_incoming_veh_df[\\'num_vehicles\\'].values\\nX, y = create_sequences(values, sequence_length)\\n\\n# Dividir los datos en conjunto de entrenamiento y prueba\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\\n\\n# Definir el modelo CNN-LSTM\\nmodel = tf.keras.Sequential([\\n    tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation=\\'relu\\', input_shape=(X_train.shape[1], 1)),\\n    tf.keras.layers.MaxPooling1D(pool_size=2),\\n    tf.keras.layers.LSTM(50, activation=\\'relu\\', return_sequences=False),\\n    tf.keras.layers.Dense(1)\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'mse\\')\\n\\n# Configuración de EarlyStopping\\nearly_stopping = EarlyStopping(\\n    monitor=\"val_loss\",  # Métrica que se monitorea\\n    patience=10,         # Número de épocas de espera sin mejoras antes de detener\\n    restore_best_weights=True,  # Restaurar los mejores pesos\\n    verbose=1          # Mostrar mensajes\\n)\\n\\n# Entrenar el modelo\\n#history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\\n\\n# Entrenamiento con EarlyStopping\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    epochs=100,\\n    batch_size=16,\\n    validation_data=(X_test, y_test),\\n    callbacks=[early_stopping],  # Incluir el callback\\n    verbose=1\\n)\\n\\n\\n# Evaluar el modelo\\nloss = model.evaluate(X_test, y_test)\\nprint(f\"Pérdida en el conjunto de prueba: {loss}\")\\n\\n# Hacer predicciones\\ny_pred = model.predict(X_test)\\n\\n# Desescalar los resultados para obtener valores originales\\ny_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\\ny_pred_rescaled = scaler.inverse_transform(y_pred)\\n\\nprint(f\"Valores originales predichos: {y_pred_rescaled[:5].flatten()}\")\\n\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "n_incoming_veh_df['num_vehicles'] = scaler.fit_transform(n_incoming_veh_df[['num_vehicles']])\n",
    "\n",
    "# Crear los datos de entrada y salida para la serie temporal\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 12  # Longitud de las secuencias\n",
    "values = n_incoming_veh_df['num_vehicles'].values\n",
    "X, y = create_sequences(values, sequence_length)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Definir el modelo CNN-LSTM\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.LSTM(50, activation='relu', return_sequences=False),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Configuración de EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "    patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "    restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "    verbose=1          # Mostrar mensajes\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "#history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Entrenamiento con EarlyStopping\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],  # Incluir el callback\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Pérdida en el conjunto de prueba: {loss}\")\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Desescalar los resultados para obtener valores originales\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
    "\n",
    "print(f\"Valores originales predichos: {y_pred_rescaled[:5].flatten()}\")\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "climaticpark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
