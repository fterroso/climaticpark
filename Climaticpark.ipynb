{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BooWVfJkNRfs",
    "outputId": "461d7b73-52f7-48c4-f3d0-26ca621a4459"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries (see configuration file .yml in github repo)\n",
    "#!pip install pybdshadow contextily folium pillow timezonefinder plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gw2SxXJJNyhf"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pybdshadow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Polygon\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytz\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpybdshadow\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#from timezonefinder import TimezoneFinder\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#import json\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#import seaborn as sns\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#from sklearn.mixture import GaussianMixture\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#import pickle\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pybdshadow'"
     ]
    }
   ],
   "source": [
    "# Library Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "#from statsmodels.tsa.api import VAR\n",
    "#from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "#from statsmodels.tools.eval_measures import rmse, aic\n",
    "#from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "#from statsmodels.stats.stattools import durbin_watson\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import pytz\n",
    "import pybdshadow\n",
    "#from timezonefinder import TimezoneFinder\n",
    "#import json\n",
    "#import seaborn as sns\n",
    "#from tabulate import tabulate\n",
    "#import plotly.express as px\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "#import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShadowModule:\n",
    "        \n",
    "        def __init__(self,lat, lon, roofs, spaces):\n",
    "            self.lat= lat\n",
    "            self.lon= lon\n",
    "            self.roofs = roofs\n",
    "            self.spaces= spaces\n",
    "        \n",
    "        \n",
    "        def compute_coverage_rates(self, days_lst):\n",
    "            \"\"\"\n",
    "            Compute coverage rates for each day in days_lst\n",
    "            \"\"\"\n",
    "\n",
    "            # DataFrames to store results\n",
    "            coverage_rates_df = pd.DataFrame()\n",
    "\n",
    "            for current_date in days_lst:\n",
    "                # Compute shadow projections for roofs\n",
    "                shadows = self._all_sunshadeshadow_sunlight(current_date)\n",
    "\n",
    "                # Calculate coverage rates\n",
    "                coverage_rates = []\n",
    "                for index, parking_space in self.spaces.iterrows():\n",
    "                    parking_space_gdf = gpd.GeoDataFrame(geometry=[parking_space.geometry])\n",
    "                    parking_space_gdf = parking_space_gdf.set_crs(epsg=4326)\n",
    "                    parking_space_gdf = parking_space_gdf.to_crs(epsg=shadows.crs.to_epsg())\n",
    "\n",
    "                    intersection = gpd.overlay(parking_space_gdf, shadows, how='intersection')\n",
    "\n",
    "                    intersection_area = intersection.geometry.area.sum()\n",
    "                    parking_space_area = parking_space_gdf.geometry.area.sum()\n",
    "\n",
    "                    coverage_rate = intersection_area / parking_space_area\n",
    "                    coverage_rates.append(coverage_rate)\n",
    "\n",
    "                coverage_rates_df[f'coverage_rate_{current_date.strftime(\"%Y-%m-%d %H:%M:%S\")}'] = coverage_rates\n",
    "\n",
    "            return coverage_rates_df\n",
    "\n",
    "            # Define function to calculate shadow and sunlight for all rooftops\n",
    "        def _all_sunshadeshadow_sunlight(date):\n",
    "            roof_projected_df= self.roofs\n",
    "            roof_projected_df['geometry'] = roof_projected_df.apply(lambda r: self._sunshadeshadow_sunlight(date, r[0]), axis=1)\n",
    "            return roof_projected_df\n",
    "    \n",
    "    \n",
    "        def _sunshadeshadow_sunlight(date, r, sunshade_height=2):\n",
    "            meanlon= r.centroid.x\n",
    "            meanlat= r.centroid.y\n",
    "            # obtain sun position\n",
    "            sunPosition = get_position(date, meanlon, meanlat)\n",
    "            if sunPosition['altitude'] < 0:\n",
    "                raise ValueError(\"Given time before sunrise or after sunset\")\n",
    "                \n",
    "            r_coords= np.array(r.exterior.coords)\n",
    "            r_coords= r_coords.reshape(1,-1,2)\n",
    "            shape = pybdshadow.utils.lonlat2aeqd(r_coords,meanlon,meanlat)\n",
    "            azimuth = sunPosition['azimuth']\n",
    "            altitude = sunPosition['altitude']\n",
    "\n",
    "            n = np.shape(shape)[0]\n",
    "            distance = sunshade_height / math.tan(altitude)\n",
    "\n",
    "            # calculate the offset of the projection position\n",
    "            lonDistance = distance * math.sin(azimuth)\n",
    "            latDistance = distance * math.cos(azimuth)\n",
    "\n",
    "            shadowShape = np.zeros((1, 5, 2))\n",
    "            shadowShape[:, :, :] += shape\n",
    "            shadowShape[:, :, 0] = shape[:, :, 0] + lonDistance\n",
    "            shadowShape[:, :, 1] = shape[:, :, 1] + latDistance\n",
    "            shadowShape = pybdshadow.utils.aeqd2lonlat(shadowShape,meanlon,meanlat)\n",
    "            p = Polygon([[p[0], p[1]] for p in shadowShape[0]])\n",
    "            return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandModule:\n",
    "    def __init__(self, entry_exit_tuples, lr=1.0, show_details=True, refresh_model=False):\n",
    "\n",
    "        print(\"Generating demand predictor...\",end=\"\")\n",
    "        \n",
    "        n_components = 2  \n",
    "        self._gm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        self._gm.fit(entry_exit_tuples[['enter_hour', 'exit_hour']])\n",
    "\n",
    "        model_path = os.path.join('_models', 'demand_cnnlstm_model.keras')\n",
    "        # Cargamos o entrenamos el modelo\n",
    "        if os.path.exists(model_path) and not refresh_model:\n",
    "            print(f\"\\n\\tLoading model from {model_path}...\", end=\"\")\n",
    "            model = load_model(model_path)\n",
    "            print(\"DONE!\")\n",
    "            self._model\n",
    "        else:\n",
    "            print(f\"\\n\\Training model...\", end=\"\")\n",
    "\n",
    "            n_incoming_veh_df= entry_exit_tuples.groupby('date').size().reset_index()\n",
    "            n_incoming_veh_df['datetime'] = pd.to_datetime(n_incoming_veh_df['date'])\n",
    "\n",
    "            # Establecer 'datetime' como índice\n",
    "            n_incoming_veh_df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Renombrar la columna 'num_vehicles'\n",
    "            n_incoming_veh_df.rename(columns={0: \"num_vehicles\"}, inplace=True)\n",
    "\n",
    "            # Eliminar columnas originales si ya no se necesitan\n",
    "            n_incoming_veh_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            n_incoming_veh_df['num_vehicles'] = scaler.fit_transform(n_incoming_veh_df[['num_vehicles']])\n",
    "\n",
    "            sequence_length = 12  # Longitud de las secuencias\n",
    "            values = n_incoming_veh_df['num_vehicles'].values\n",
    "            X, y = self._create_unidimensional_sequences(values, sequence_length)\n",
    "\n",
    "            # Dividir los datos en conjunto de entrenamiento y prueba\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-lr, random_state=42)\n",
    "\n",
    "            # Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\n",
    "            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "            X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "            # Definir el modelo CNN-LSTM\n",
    "            self._model = Sequential([\n",
    "                Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                LSTM(50, activation='relu', return_sequences=False),\n",
    "                Dense(1)\n",
    "            ])\n",
    "\n",
    "            self._model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "            _verbose= 0\n",
    "            if show_details:\n",
    "                _verbose= 1\n",
    "            # Configuración de EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "                patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "                restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "                verbose=_verbose          # Mostrar mensajes\n",
    "            )\n",
    "\n",
    "            # Entrenamiento con EarlyStopping\n",
    "            self._model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=100,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping],  # Incluir el callback\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            self._last_sequence = X_test[-1].flatten() \n",
    "            self._model.save(model_path)\n",
    "\n",
    "            print(\"DONE!\")\n",
    "\n",
    "    # Crear los datos de entrada y salida para la serie temporal\n",
    "    def _create_unidimensional_sequences(self, data, sequence_length):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - sequence_length):\n",
    "            X.append(data[i:i + sequence_length])\n",
    "            y.append(data[i + sequence_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def _predict_future_occupancy(self, n_days_ahead):\n",
    "\n",
    "        future_predictions = []\n",
    "        current_sequence = self._last_sequence.copy()\n",
    "\n",
    "        for _ in range(n_days_ahead*24):\n",
    "            # Redimensionar la secuencia actual para que sea compatible con el modelo\n",
    "            input_data = np.array(current_sequence).reshape((1, self.sequence_length, 1))\n",
    "            \n",
    "            # Predecir el siguiente valor\n",
    "            next_pred = self._model.predict(input_data, verbose=0)[0][0]\n",
    "            \n",
    "            # Guardar el valor predicho (desnormalizado)\n",
    "            future_predictions.append(self.scaler.inverse_transform([[next_pred]])[0][0])\n",
    "\n",
    "            # Actualizar la secuencia de entrada con la nueva predicción\n",
    "            current_sequence = np.append(current_sequence[1:], next_pred)\n",
    "\n",
    "        return future_predictions\n",
    "\n",
    "    def _generate_entry_and_exit_hours(self, n_tuples):\n",
    "        new_samples, _ = gm.sample(n_samples=n_tuples)\n",
    "        return [[round(x[0]),round(x[1])] for x in new_samples]\n",
    "    \n",
    "    def generate_occupancy(self, num_days_ahead):\n",
    "        \"\"\"\n",
    "        Simulates vehicle parking occupancy and generates a table of occupancy information.\n",
    "        \"\"\"\n",
    "        n_vehicles_per_hour = self._predict_future_occupancy(num_days_ahead)\n",
    "        \n",
    "        for n_vehicles in n_vehicles_per_hour: \n",
    "        \n",
    "            new_samples, _ = gm.sample(n_samples=n_vehicles)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmbientModule:\n",
    "    \n",
    "    def __init__(self, lat, lon, cabin_temperatures):\n",
    "        self.lat= lat\n",
    "        self.lon= lon\n",
    "\n",
    "\n",
    "    def train_cabin_temperature_model(self, combined_temp_df, lr=1.0, show_details=False, refresh_model=False):\n",
    "        \"\"\"\n",
    "        Trains a LSTM model using the combined temperature data.\n",
    "\n",
    "        :param combined_temp_data: DataFrame containing combined temperature data\n",
    "        \"\"\"\n",
    "        model_path = os.path.join('_models', 'cabintemp_lstm_model.keras')\n",
    "\n",
    "        init_date =  self.cabin_temperatures.index[0].date()\n",
    "        final_date =  self.cabin_temperatures.index[-1].date()\n",
    "\n",
    "        # Normalización de los datos\n",
    "        scaler_a = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler_b = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "        data = combined_temp_df.copy()\n",
    "        data[\"temperature_2m\"] = scaler_a.fit_transform(combined_temp_df[\"temperature_2m\"].values.reshape(-1, 1))\n",
    "        data[\"recorded_cabin_temp\"] = scaler_b.fit_transform(combined_temp_df[\"recorded_cabin_temp\"].values.reshape(-1, 1))\n",
    "\n",
    "        look_back = 12  # Número de pasos anteriores a considerar\n",
    "        X, y = self._create_sequences(data.values, look_back)\n",
    "\n",
    "        # Dividimos en entrenamiento y prueba\n",
    "        train_size = int(len(X) * lr)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "        # Redimensionamos las entradas para LSTM [samples, time steps, features]\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        model = None\n",
    "        # Cargamos o entrenamos el modelo\n",
    "        if os.path.exists(model_path) and not refresh_model:\n",
    "            print(f\"\\n\\t Loading model from {model_path}...\")\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print(f\"\\n\\tTraining and saving model in {model_path}...\")\n",
    "\n",
    "            _verbose= 0\n",
    "            if show_details:\n",
    "              _verbose= 1\n",
    "            # Configuración de EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "                patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "                restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "                verbose=_verbose           # Mostrar mensajes\n",
    "            )\n",
    "\n",
    "\n",
    "            # Construcción del modelo LSTM\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(50, input_shape=(look_back, 1)))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "            # Entrenamiento con EarlyStopping\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=1000,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping],  # Incluir el callback\n",
    "                verbose=1\n",
    "            )\n",
    "        model.save(model_path)\n",
    "\n",
    "        return model, scaler_a, scaler_b\n",
    "\n",
    "    # Función para obtener datos horarios históricos de Open-Meteo API\n",
    "    def _fetch_historical_temperature(self, start_date, end_date):\n",
    "          \"\"\"\n",
    "          Obtiene datos horarios históricos de Open-Meteo API.\n",
    "\n",
    "          Args:\n",
    "          - latitude (float): Latitud de la ubicación.\n",
    "          - longitude (float): Longitud de la ubicación.\n",
    "          - start_date (str): Fecha de inicio en formato YYYY-MM-DD.\n",
    "          - end_date (str): Fecha de fin en formato YYYY-MM-DD.\n",
    "          - parameters (list): Variables meteorológicas a consultar, ej. ['temperature_2m', 'humidity_2m'].\n",
    "\n",
    "          Returns:\n",
    "          - pd.DataFrame: Datos meteorológicos horarios como DataFrame.\n",
    "          \"\"\"\n",
    "          base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "\n",
    "          # Crear el payload para la solicitud\n",
    "          payload = {\n",
    "              \"latitude\": self.lat,\n",
    "              \"longitude\": self.lon,\n",
    "              \"start_date\": start_date,\n",
    "              \"end_date\": end_date,\n",
    "              \"hourly\": 'temperature_2m',\n",
    "              \"timezone\": \"auto\"\n",
    "          }\n",
    "\n",
    "          # Realizar la solicitud a la API\n",
    "          response = requests.get(base_url, params=payload)\n",
    "\n",
    "          if response.status_code == 200:\n",
    "              # Convertir la respuesta JSON a un DataFrame\n",
    "              data = response.json()\n",
    "              if \"hourly\" in data:\n",
    "                  df = pd.DataFrame(data[\"hourly\"])\n",
    "                  df['time'] = pd.to_datetime(df['time'])\n",
    "                  df= df.set_index('time')\n",
    "                  return df\n",
    "              else:\n",
    "                  print(\"No se encontraron datos en la respuesta.\")\n",
    "                  return pd.DataFrame()\n",
    "          else:\n",
    "              print(f\"Error en la solicitud: {response.status_code} - {response.text}\")\n",
    "              return pd.DataFrame()\n",
    "          \n",
    "\n",
    "    def _forecast_uncovered_cabin_temperatures(self, ambient_temp):\n",
    "        ambient_temp_scaled = self.temp_scaler.fit_transform(ambient_temp)\n",
    "        y_pred = self.cabin_temp_model.predict(ambient_temp_scaled)\n",
    "        y_pred_rescaled = self.cabin_temp_scaler.inverse_transform(y_pred)\n",
    "        return y_pred_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3haCJlsN8hF"
   },
   "outputs": [],
   "source": [
    "class ClimaticPark:\n",
    "    def __init__(self, file_name_lots='data/parking_lots.geojson',\n",
    "                 file_name_roofs='data/parking_roofs.geojson',\n",
    "                 file_name_coords='data/parking_coordinates.csv',\n",
    "                 file_name_gates='data/gates_coordinates.csv',\n",
    "                 file_name_mapcenter='data/map_center_coordinates.csv',\n",
    "                 file_name_cabintem='data/historical_cabin_temp.csv',\n",
    "                 file_name_tuples='data/entry_exit_tuples.csv'):\n",
    "        \"\"\"\n",
    "        Initializes the ClimaticPark object by loading all necessary files.\n",
    "        \"\"\"\n",
    "        # Load GeoJSON files for lots and roofs\n",
    "        self.lots_data = self._load_geojson(file_name_lots)\n",
    "        self.roofs_data = self._load_geojson(file_name_roofs)\n",
    "\n",
    "        # Add 'height' column to roofs_data with a value of 1 for all rows\n",
    "        if (self.lots_data is not None) and ('height' not in self.lots_data.columns):\n",
    "            self.lots_data['height'] = 1  # Assuming a default height of 1\n",
    "\n",
    "        # Add 'height' column to roofs_data with a value of 1 for all rows\n",
    "        if (self.roofs_data is not None) and ('height' not in self.roofs_data.columns):\n",
    "            self.roofs_data['height'] = 1  # Assuming a default height of 1\n",
    "\n",
    "        # Load CSV files for coordinates, historical data, and additional data\n",
    "        self.coords_data = self._load_csv(file_name_coords)\n",
    "        self.gates_data = self._load_csv(file_name_gates)\n",
    "      \n",
    "        self.data_no_roof =  ClimaticPark.load_csv(os.path.join('data', 'cabin_temperature_no_roof.csv'))\n",
    "        self.data_no_roof['coverage']=0\n",
    "        self.data_roof =  ClimaticPark.load_csv(os.path.join('data', 'cabin_temperature_w_roof.csv'))\n",
    "        self.data_roof['coverage']=1\n",
    "\n",
    "        self.recorded_cabin_temp = ClimaticPark.read_csv(file_name_cabintem, index_col=0)\n",
    "        # Convertir la columna DateTime a tipo datetime\n",
    "        self.recorded_cabin_temp['DateTime'] = pd.to_datetime(self.recorded_cabin_temp['DateTime'])\n",
    "        # Establecer la columna DateTime como índice\n",
    "        self.recorded_cabin_temp.set_index('DateTime', inplace=True)\n",
    "        # Remuestrear los datos para obtener una frecuencia de 1 hora (calculando la media)\n",
    "        self.recorded_cabin_temp = self.recorded_cabin_temp.resample('h').mean()\n",
    "\n",
    "        self.entry_exit_tuples= pd.read_csv(os.path.join('data', 'entry_exit_tuples_clean.csv'),\n",
    "                                          index_col=0, dtype={'id_subject':str}, parse_dates=['date'])\n",
    "        \n",
    "        os.makedirs('_models', exist_ok=True)\n",
    "        self.cabin_temp_model = None\n",
    "        self.cabin_coverage_model= None\n",
    "        self.demand_module = DemandModule(self.entry_exit_tuples)\n",
    "        \n",
    "        lat = self.coords_data['latitude'].iloc[0]  \n",
    "        lon = self.coords_data['longitude'].iloc[0]\n",
    "        self.shadow_module = ShadowModule(lat, lon, self.roofs_data, self.lots_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_geojson(file_name):\n",
    "        \"\"\"\n",
    "        Loads a GeoJSON file into a GeoDataFrame.\n",
    "        \"\"\"\n",
    "        if file_name:\n",
    "            return gpd.read_file(file_name)\n",
    "        else:\n",
    "            print(f\"No GeoJSON file provided for {file_name}.\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_csv(file_name):\n",
    "        \"\"\"\n",
    "        Loads a CSV file into a DataFrame.\n",
    "        \"\"\"\n",
    "        if file_name:\n",
    "            return pd.read_csv(file_name)\n",
    "        else:\n",
    "            print(f\"No CSV file provided for {file_name}.\")\n",
    "            return None\n",
    "\n",
    "    def prepare_simulation(self, lr=0.8, display_details=False):\n",
    "        if not self.coords_data.empty:\n",
    "            latitude = self.coords_data['latitude'].iloc[0]  # Get the first coordinate\n",
    "            longitude = self.coords_data['longitude'].iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"The coordinates file is empty or not formatted correctly.\")\n",
    "\n",
    "        #init_date =  self.recorded_cabin_temp.index[0].date()\n",
    "        #final_date =  self.recorded_cabin_temp.index[-1].date()\n",
    "\n",
    "        # Process temperature data\n",
    "        print(\"Fetching historical weather conditions of the TPL...\",end=\"\")\n",
    "        ambient_temp_df = self._fetch_historical_temperature(latitude, longitude, init_date, final_date)\n",
    "        print(\"DONE!\")\n",
    "        combined_temp_df= pd.concat([ambient_temp_df, self.recorded_cabin_temp], axis=1)\n",
    "        combined_temp_df = combined_temp_df.dropna()\n",
    "\n",
    "\n",
    "        print(\"Training cabin temperature predictors...\",end=\"\")\n",
    "        self.cabin_temp_model, self.temp_scaler, self.cabin_temp_scaler = self._train_cabin_temperature_model(combined_temp_df, lr, display_details)\n",
    "        self.cabin_coverage_model= self._train_cabin_temperature_and_coverage_model()\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        print(\"Training demand predictor...\",end=\"\")\n",
    "        self.demand_model = self._train_demand_model(lr)\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        print(\"Training GMM of entry-exit hours...\",end=\"\")\n",
    "        self.gmm_demand= self._train_entry_exit_gmm()\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        print(\"Simulation ready to go!!\")\n",
    "\n",
    "    def launch_simulation(self, n_days_ahead, display_details=True):\n",
    "        \"\"\"\n",
    "        Lauch simulation for n_days_ahead \n",
    "        \"\"\"\n",
    "\n",
    "        if not self.coords_data.empty:\n",
    "            latitude = self.coords_data['latitude'].iloc[0]  # Get the first coordinate\n",
    "            longitude = self.coords_data['longitude'].iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"The coordinates file is empty or not formatted correctly.\")\n",
    "\n",
    "        init_day = self.entry_exit_tuples.max('date').date()\n",
    "\n",
    "        date_lst = [init_day + timedelta(days=i) for i in range(n_days_ahead)]\n",
    "        days_lst = [date.date() for date in date_lst]\n",
    "\n",
    "        ambient_temp= self._fetch_historical_temperature(self, latitude, longitude, init_day, days_lst[-1])\n",
    "        ambient_temp= ambient_temp.loc[init_date_dt:final_date_dt]\n",
    "        uncovered_cabin_temp= self._forecast_uncovered_cabin_temperatures(ambient_temp)\n",
    "        \n",
    "        coverage_rates= self.shadow_module.compute_coverage_rates(days_lst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Preparamos el dataset para secuencias\n",
    "    def _create_sequences(self, data, look_back):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - look_back):\n",
    "            X.append(data[i:i + look_back, 0])  # Secuencias de la variable 'a'\n",
    "            y.append(data[i + look_back, 1])  # Predicción futura de la variable 'b'\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _train_cabin_temperature_and_coverage_model(self):\n",
    "\n",
    "        temp_data = pd.concat([self.data_no_roof, self.data_roof], axis=0)\n",
    "\n",
    "        X= temp_data['T temp_ext coverage'.split()].values\n",
    "        y=  temp_data['temp_int'].values\n",
    "\n",
    "        clf = LinearRegression().fit(X, y)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "okbjHghGOYix",
    "outputId": "c4f6263c-ddf9-4dfa-d05f-59486896d7c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical weather conditions of the TPL...DONE!\n",
      "Training cabin temperature predictor...Loading model from _models\\cabintemp_lstm_model.keras...\n",
      "DONE!\n",
      "Generando modelo de temperatura de cabina y cover rates...DONE!\n",
      "Generating demand predictor...\n",
      "\tLoading model from _models\\demand_cnnlstm_model.keras...DONE!\n",
      "Training gaussian mixture model of entry-exit hours...DONE!\n",
      "Simulation ready to go!!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m park \u001b[38;5;241m=\u001b[39m ClimaticPark() \u001b[38;5;66;03m#default parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m park\u001b[38;5;241m.\u001b[39mprepare_simulation()\n\u001b[1;32m----> 3\u001b[0m park\u001b[38;5;241m.\u001b[39mlaunch_simulation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-10-24 07:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 115\u001b[0m, in \u001b[0;36mClimaticPark.launch_simulation\u001b[1;34m(self, n_days_ahead, display_details)\u001b[0m\n\u001b[0;32m    112\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe coordinates file is empty or not formatted correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Obtenemos solo la fecha (sin la hora)\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m init_day \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_exit_tuples_clean\u001b[38;5;241m.\u001b[39mmax[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdate()\n\u001b[0;32m    117\u001b[0m date_lst \u001b[38;5;241m=\u001b[39m [init_day \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_days_ahead)]\n\u001b[0;32m    118\u001b[0m days_lst \u001b[38;5;241m=\u001b[39m [date\u001b[38;5;241m.\u001b[39mdate() \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m date_lst]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "park = ClimaticPark() #default parameters\n",
    "park.prepare_simulation()\n",
    "park.launch_simulation('2022-10-24 07:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eB0DJwZqK0N"
   },
   "outputs": [],
   "source": [
    "raw_park_records_df= pd.read_csv(os.path.join('data', 'entry_exit_tuples_clean.csv'), index_col=0,\n",
    "                              dtype={'id_subject':str}, parse_dates=['date'])\n",
    "raw_park_records_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ss7TwqaHPZyM"
   },
   "outputs": [],
   "source": [
    "init_day= raw_park_records_df['date'].max()\n",
    "print(init_day)\n",
    "n_days = 4\n",
    "date_lst = [init_day + timedelta(days=i) for i in range(n_days)]\n",
    "days_lst= [date.date() for date in date_lst]\n",
    "\n",
    "print(days_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJlj2AcmdH5i"
   },
   "outputs": [],
   "source": [
    "n_components = 2  # Número de componentes de la mezcla\n",
    "gm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "gm.fit(raw_park_records_df[['enter_hour', 'exit_hour']])\n",
    "\n",
    "new_samples, _ = gm.sample(n_samples=100)\n",
    "[[round(x[0]),round(x[1])] for x in new_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b13rN6MlMOGm"
   },
   "outputs": [],
   "source": [
    "n_incoming_veh_df= raw_park_records_df.groupby('date').size().reset_index()\n",
    "# Crear columna datetime a partir de 'date' y 'enter_hour'\n",
    "n_incoming_veh_df['datetime'] = pd.to_datetime(n_incoming_veh_df['date'])\n",
    "\n",
    "# Establecer 'datetime' como índice\n",
    "n_incoming_veh_df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Renombrar la columna 'num_vehicles'\n",
    "n_incoming_veh_df.rename(columns={0: \"num_vehicles\"}, inplace=True)\n",
    "\n",
    "# Eliminar columnas originales si ya no se necesitan\n",
    "n_incoming_veh_df.drop(columns=[\"date\"], inplace=True)\n",
    "n_incoming_veh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNIvSE1PM5IM"
   },
   "outputs": [],
   "source": [
    "ax=n_incoming_veh_df.plot(figsize=(15,5), grid=True)\n",
    "ax.set_xlabel('Date',fontsize=20);\n",
    "ax.set_ylabel('Num. of hourly users',fontsize=20);\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.tick_params(axis='x', labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFXCAlxhQtOT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "n_incoming_veh_df['num_vehicles'] = scaler.fit_transform(n_incoming_veh_df[['num_vehicles']])\n",
    "\n",
    "# Crear los datos de entrada y salida para la serie temporal\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 12  # Longitud de las secuencias\n",
    "values = n_incoming_veh_df['num_vehicles'].values\n",
    "X, y = create_sequences(values, sequence_length)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Definir el modelo CNN-LSTM\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.LSTM(50, activation='relu', return_sequences=False),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Configuración de EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "    patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "    restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "    verbose=1          # Mostrar mensajes\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "#history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Entrenamiento con EarlyStopping\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],  # Incluir el callback\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Pérdida en el conjunto de prueba: {loss}\")\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Desescalar los resultados para obtener valores originales\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
    "\n",
    "print(f\"Valores originales predichos: {y_pred_rescaled[:5].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcuemtgobikt"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Función para obtener datos horarios históricos de Open-Meteo API\n",
    "def get_historical_weather_hourly(latitude, longitude, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Obtiene datos horarios históricos de Open-Meteo API.\n",
    "\n",
    "    Args:\n",
    "    - latitude (float): Latitud de la ubicación.\n",
    "    - longitude (float): Longitud de la ubicación.\n",
    "    - start_date (str): Fecha de inicio en formato YYYY-MM-DD.\n",
    "    - end_date (str): Fecha de fin en formato YYYY-MM-DD.\n",
    "    - parameters (list): Variables meteorológicas a consultar, ej. ['temperature_2m', 'humidity_2m'].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Datos meteorológicos horarios como DataFrame.\n",
    "    \"\"\"\n",
    "    base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "\n",
    "    # Crear el payload para la solicitud\n",
    "    payload = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": 'temperature_2m',\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "\n",
    "    # Realizar la solicitud a la API\n",
    "    response = requests.get(base_url, params=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Convertir la respuesta JSON a un DataFrame\n",
    "        data = response.json()\n",
    "        if \"hourly\" in data:\n",
    "            df = pd.DataFrame(data[\"hourly\"])\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No se encontraron datos en la respuesta.\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"Error en la solicitud: {response.status_code} - {response.text}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "ambient_temperatures_df = get_historical_weather_hourly(park.coords_data['latitude'].iloc[0], park.coords_data['longitude'].iloc[0], init_date, final_date)\n",
    "ambient_temperatures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utVwpNvnk-iP"
   },
   "outputs": [],
   "source": [
    "ambient_temperatures_df['time'] = pd.to_datetime(ambient_temperatures_df['time'])\n",
    "\n",
    "ambient_temperatures_df= ambient_temperatures_df.set_index('time')\n",
    "ambient_temperatures_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "climaticpark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
