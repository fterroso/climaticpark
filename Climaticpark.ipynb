{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BooWVfJkNRfs",
    "outputId": "461d7b73-52f7-48c4-f3d0-26ca621a4459"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries (see configuration file climaticpark_env.yml in github repo)\n",
    "#!pip install pybdshadow contextily folium pillow timezonefinder plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Gw2SxXJJNyhf"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'branca'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbranca\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#from statsmodels.tsa.api import VAR\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#from statsmodels.tsa.stattools import adfuller, grangercausalitytests\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#from statsmodels.tools.eval_measures import rmse, aic\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#from statsmodels.tsa.vector_ar.vecm import coint_johansen\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#from statsmodels.stats.stattools import durbin_watson\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#import folium\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'branca'"
     ]
    }
   ],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime as dt \n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "import branca\n",
    "\n",
    "#from statsmodels.tsa.api import VAR\n",
    "#from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "#from statsmodels.tools.eval_measures import rmse, aic\n",
    "#from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "#from statsmodels.stats.stattools import durbin_watson\n",
    "#import folium\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "#import pytz\n",
    "#import pybdshadow\n",
    "#from timezonefinder import TimezoneFinder\n",
    "#import json\n",
    "#import seaborn as sns\n",
    "#from tabulate import tabulate\n",
    "#import plotly.express as px\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "#import pickle\n",
    "import os\n",
    "\n",
    "from suncalc import get_position\n",
    "from pyproj import CRS,Transformer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShadowModule:\n",
    "        \n",
    "    def __init__(self,lat, lon, roofs, spaces):\n",
    "        self.lat= lat\n",
    "        self.lon= lon\n",
    "        self.roofs = roofs\n",
    "        self.spaces= spaces\n",
    "    \n",
    "    \n",
    "    def compute_coverage_rates(self, days_lst):\n",
    "        \"\"\"\n",
    "        Compute coverage rates for each day in days_lst\n",
    "        \"\"\"\n",
    "\n",
    "        # DataFrames to store results\n",
    "        #coverage_rates_df = pd.DataFrame()\n",
    "\n",
    "        #print(self.spaces.head())\n",
    "        #print(self.spaces.info())\n",
    "        #print(self.spaces.geometry)\n",
    "        #print(self.spaces['geometry'].apply(type).unique())  # Asegúrate de que todas sean geometrías válidas\n",
    "\n",
    "        coverage_rates_gdf = gpd.GeoDataFrame(geometry=self.spaces.geometry)\n",
    "        coverage_rates_gdf = coverage_rates_gdf.set_crs(epsg=4326)\n",
    "        #coverage_rates_gdf = coverage_rates_gdf.to_crs(epsg=self.spaces.crs.to_epsg())\n",
    "\n",
    "        for current_date in days_lst:\n",
    "            # Compute shadow projections for roofs\n",
    "\n",
    "            for hour in range(6,22):\n",
    "                \n",
    "                date_hour = dt.combine(current_date, datetime.time(hour, 0)) \n",
    "\n",
    "                #try:\n",
    "                shadows_gdf = self._all_sunshadeshadow_sunlight(date_hour)\n",
    "\n",
    "                # Calculate coverage rates\n",
    "                intersection = gpd.overlay(coverage_rates_gdf, shadows_gdf, how='intersection')\n",
    "\n",
    "                \"\"\"\n",
    "                print(\"*\"*8,\"shadows_gdf\",\"*\"*8)\n",
    "                print(shadows_gdf.info())\n",
    "                print(shadows_gdf.head(2))                \n",
    "                print(\"*\"*8,\"intersection\",\"*\"*8)\n",
    "                print(intersection.info())\n",
    "                print(intersection.head(2))\n",
    "                print(\"*\"*8,\"coverage_rates_gdf\",\"*\"*8,)\n",
    "                print(coverage_rates_gdf.info())\n",
    "                print(coverage_rates_gdf.head(2))                \n",
    "                print(\"*\"*8)\n",
    "                \"\"\"\n",
    "\n",
    "                coverage_rates= []\n",
    "                for index, parking_space in coverage_rates_gdf.iterrows():\n",
    "                    #print(parking_space)\n",
    "                    space_total_area = parking_space.geometry.area\n",
    "                    space_shadow_area = intersection.loc[index, \"geometry\"].area if index in intersection.index else 0 #intersection.loc[index,\"geometry\"].area\n",
    "                    space_coverage= space_shadow_area / space_total_area\n",
    "                    #print(index, space_total_area,space_shadow_area,space_coverage)\n",
    "                    #print(\"*\"*8)\n",
    "                    coverage_rates.append(space_coverage)\n",
    "\n",
    "                #intersection_area = intersection.geometry.area.sum()\n",
    "                #parking_space_area = coverage_rates_gdf.geometry.area.sum()\n",
    "\n",
    "                #coverage_rates = intersection_area / parking_space_area\n",
    "                #print(coverage_rates)\n",
    "\n",
    "                \"\"\"\n",
    "                coverage_rates = []\n",
    "                for index, parking_space in self.spaces.iterrows():\n",
    "                    parking_space_gdf = gpd.GeoDataFrame(geometry=self..geometry)\n",
    "                    parking_space_gdf = parking_space_gdf.set_crs(epsg=4326)\n",
    "                    parking_space_gdf = parking_space_gdf.to_crs(epsg=shadows_gdf.crs.to_epsg())\n",
    "\n",
    "                    intersection = gpd.overlay(parking_space_gdf, shadows_gdf, how='intersection')\n",
    "\n",
    "                    intersection_area = intersection.geometry.area.sum()\n",
    "                    parking_space_area = parking_space_gdf.geometry.area.sum()\n",
    "\n",
    "                    coverage_rate = intersection_area / parking_space_area\n",
    "                    coverage_rates.append(coverage_rate)\n",
    "                \"\"\"\n",
    "\n",
    "                coverage_rates_gdf[f'coverage_rate_{date_hour.strftime(\"%Y-%m-%d %H:%M\")}'] = coverage_rates\n",
    "                #except Exception as e:\n",
    "                #    print(f\"ERROR:: ShadowModule compute_coverage_rates: {e}\")\n",
    "        self._coverage_rates = coverage_rates_gdf\n",
    "        return coverage_rates_gdf\n",
    "\n",
    "        # Define function to calculate shadow and sunlight for all rooftops\n",
    "    def _all_sunshadeshadow_sunlight(self, date):\n",
    "        roof_projected_df= self.roofs.copy()\n",
    "        roof_projected_df['geometry'] = roof_projected_df['geometry'].apply(lambda r: self._sunshadeshadow_sunlight(np.datetime64(date), r))\n",
    "        return roof_projected_df\n",
    "\n",
    "\n",
    "    def _sunshadeshadow_sunlight(self, date, r, sunshade_height=2):\n",
    "        meanlon= r.centroid.y\n",
    "        meanlat= r.centroid.x\n",
    "        # obtain sun position\n",
    "        sunPosition = get_position(date, meanlon, meanlat)\n",
    "        if sunPosition['altitude'] < 0:\n",
    "            raise ValueError(\"Given time before sunrise or after sunset\")\n",
    "            \n",
    "        r_coords= np.array(r.exterior.coords)\n",
    "        r_coords= r_coords.reshape(1,-1,2)\n",
    "        shape = ShadowModule.lonlat2aeqd(r_coords,meanlon,meanlat)\n",
    "        azimuth = sunPosition['azimuth']\n",
    "        altitude = sunPosition['altitude']\n",
    "\n",
    "        n = np.shape(shape)[0]\n",
    "        distance = sunshade_height / math.tan(altitude)\n",
    "\n",
    "        # calculate the offset of the projection position\n",
    "        lonDistance = distance * math.sin(azimuth)\n",
    "        latDistance = distance * math.cos(azimuth)\n",
    "\n",
    "        shadowShape = np.zeros((1, 5, 2))\n",
    "        shadowShape[:, :, :] += shape\n",
    "        shadowShape[:, :, 0] = shape[:, :, 0] + lonDistance\n",
    "        shadowShape[:, :, 1] = shape[:, :, 1] + latDistance\n",
    "        shadowShape = ShadowModule.aeqd2lonlat(shadowShape,meanlon,meanlat)\n",
    "        p = Polygon([[p[0], p[1]] for p in shadowShape[0]])\n",
    "        return p\n",
    "    \n",
    "    @staticmethod\n",
    "    def lonlat2aeqd(lonlat, center_lon, center_lat):\n",
    "        epsg = CRS.from_proj4(\"+proj=aeqd +lat_0=\"+str(center_lat) +\n",
    "                            \" +lon_0=\"+str(center_lon)+\" +datum=WGS84\")\n",
    "        transformer = Transformer.from_crs(\"EPSG:4326\", epsg, always_xy=True)\n",
    "        proj_coords = transformer.transform(lonlat[:, :, 0], lonlat[:, :, 1])\n",
    "        proj_coords = np.array(proj_coords).transpose([1, 2, 0])\n",
    "        return proj_coords\n",
    "    \n",
    "    @staticmethod\n",
    "    def aeqd2lonlat(proj_coords,meanlon,meanlat):\n",
    "        epsg = CRS.from_proj4(\"+proj=aeqd +lat_0=\"+str(meanlat)+\" +lon_0=\"+str(meanlon)+\" +datum=WGS84\")\n",
    "        transformer = Transformer.from_crs( epsg,\"EPSG:4326\",always_xy = True)\n",
    "        lonlat = transformer.transform(proj_coords[:,:,0], proj_coords[:,:,1])\n",
    "        lonlat = np.array(lonlat).transpose([1,2,0])\n",
    "        return lonlat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandModule:\n",
    "    def __init__(self, entry_exit_tuples):        \n",
    "        self._entry_exit_tuples= entry_exit_tuples\n",
    "\n",
    "    def train_demand_predictors(self, lr=0.9, sequence_length= 12, show_details=True, refresh_model=False):\n",
    "        self._sequence_length= sequence_length\n",
    "\n",
    "        #self._gm = GaussianMixture(n_components=self._n_components, random_state=42)\n",
    "        #self._gm.fit(self._entry_exit_tuples[['enter_hour', 'exit_hour']])\n",
    "\n",
    "        self._gmm_models={}\n",
    "        for hour, group in self._entry_exit_tuples.groupby('enter_hour'):\n",
    "            X = group[\"exit_hour\"].values  # Extraer los datos de las características para este grupo\n",
    "            if X.shape[0]== 1:\n",
    "                X= X.reshape(1, -1)\n",
    "            else:\n",
    "                X= X.reshape(-1,1)\n",
    "            if X.shape[0]>=2:\n",
    "                # Crear y ajustar el modelo GMM; ajusta n_components según tus necesidades\n",
    "                gmm = GaussianMixture(n_components=1, random_state=42)\n",
    "                gmm.fit(X)\n",
    "                self._gmm_models[hour] = gmm\n",
    "\n",
    "        model_path = os.path.join('_models', 'demand_cnnlstm_model.keras')\n",
    "        # Cargamos o entrenamos el modelo\n",
    "\n",
    "        n_incoming_veh_df= self._entry_exit_tuples.groupby('date').size().reset_index()\n",
    "        n_incoming_veh_df['datetime'] = pd.to_datetime(n_incoming_veh_df['date'])\n",
    "\n",
    "        # Establecer 'datetime' como índice\n",
    "        n_incoming_veh_df.set_index('datetime', inplace=True)\n",
    "\n",
    "        # Renombrar la columna 'num_vehicles'\n",
    "        n_incoming_veh_df.rename(columns={0: \"num_vehicles\"}, inplace=True)\n",
    "\n",
    "        # Eliminar columnas originales si ya no se necesitan\n",
    "        n_incoming_veh_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "        self._scaler = MinMaxScaler()\n",
    "        n_incoming_veh_df['num_vehicles'] = self._scaler.fit_transform(n_incoming_veh_df[['num_vehicles']])\n",
    "\n",
    "        values = n_incoming_veh_df['num_vehicles'].values\n",
    "        X, y = self._create_unidimensional_sequences(values, self._sequence_length)\n",
    "\n",
    "        # Dividir los datos en conjunto de entrenamiento y prueba\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-lr, random_state=42)\n",
    "\n",
    "        # Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        self._last_sequence = X_test[-1].flatten() \n",
    "\n",
    "        if os.path.exists(model_path) and not refresh_model:\n",
    "            print(f\"\\n\\tLoading demand predictor from {model_path}...\", end=\"\")\n",
    "            self._model = load_model(model_path)\n",
    "        else: \n",
    "            print(f\"\\n\\tTraining demand predictor...\", end=\"\")\n",
    "\n",
    "            # Definir el modelo CNN-LSTM\n",
    "            self._model = Sequential([\n",
    "                Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                LSTM(50, activation='relu', return_sequences=False),\n",
    "                Dense(1)\n",
    "            ])\n",
    "\n",
    "            self._model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "            _verbose= 0\n",
    "            if show_details:\n",
    "                _verbose= 1\n",
    "            # Configuración de EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "                patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "                restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "                verbose=_verbose          # Mostrar mensajes\n",
    "            )\n",
    "\n",
    "            # Entrenamiento con EarlyStopping\n",
    "            self._model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=1000,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping],  # Incluir el callback\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            self._model.save(model_path)\n",
    "\n",
    "        print(\"DONE!\")\n",
    "\n",
    "    # Crear los datos de entrada y salida para la serie temporal\n",
    "    def _create_unidimensional_sequences(self, data, sequence_length):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - sequence_length):\n",
    "            X.append(data[i:i + sequence_length])\n",
    "            y.append(data[i + sequence_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def _predict_occupancy(self, n_days_ahead):\n",
    "\n",
    "        future_predictions = []\n",
    "        current_sequence = self._last_sequence.copy()\n",
    "\n",
    "        for _ in range(n_days_ahead*24):\n",
    "            # Redimensionar la secuencia actual para que sea compatible con el modelo\n",
    "            input_data = np.array(current_sequence).reshape((1, self._sequence_length, 1))\n",
    "            \n",
    "            # Predecir el siguiente valor\n",
    "            next_pred = self._model.predict(input_data, verbose=0)[0][0]\n",
    "            \n",
    "            # Guardar el valor predicho (desnormalizado)\n",
    "            future_predictions.append(self._scaler.inverse_transform([[next_pred]])[0][0])\n",
    "\n",
    "            # Actualizar la secuencia de entrada con la nueva predicción\n",
    "            current_sequence = np.append(current_sequence[1:], next_pred)\n",
    "\n",
    "        return future_predictions\n",
    "\n",
    "    def _generate_entry_and_exit_hours(self, n_tuples):\n",
    "        new_samples, _ = gm.sample(n_samples=n_tuples)\n",
    "        return [[round(x[0]),round(x[1])] for x in new_samples]\n",
    "    \n",
    "    def generate_occupancy(self, num_days_ahead):\n",
    "        \"\"\"\n",
    "        Simulates vehicle parking occupancy and generates a table of occupancy information.\n",
    "        \"\"\"\n",
    "\n",
    "        n_vehicles_per_hour = self._predict_occupancy(num_days_ahead)\n",
    "        simulated_occupancy=[]\n",
    "        hour= 0\n",
    "        for n_vehicles in n_vehicles_per_hour: \n",
    "            if hour in self._gmm_models:\n",
    "                gm = self._gmm_models[hour]      \n",
    "                new_samples, _ = gm.sample(n_samples=n_vehicles)\n",
    "                simulated_occupancy.append([(hour, round(x[0])) for x in new_samples])\n",
    "            hour = (hour+1) % 24\n",
    "        return simulated_occupancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmbientModule:\n",
    "    \n",
    "    def __init__(self, lat, lon):\n",
    "        self.lat= lat\n",
    "        self.lon= lon\n",
    "\n",
    "    def train_ambient_temperature_model(self, start_date, end_date, lr=0.9, show_details=False, refresh_model=False):\n",
    "        \"\"\"\n",
    "        Trains a LSTM model using the combined temperature data.\n",
    "\n",
    "        :param combined_temp_data: DataFrame containing combined temperature data\n",
    "        \"\"\"\n",
    "\n",
    "        ambient_temperaure_df= self._fetch_historical_temperature(start_date, end_date)\n",
    "        model_path = os.path.join('_models', 'cabintemp_lstm_model.keras')\n",
    "\n",
    "        # Normalización de los datos\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        #scaler_b = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "        data = ambient_temperaure_df.copy()\n",
    "        data[\"temperature_2m\"] = scaler.fit_transform(ambient_temperaure_df[\"temperature_2m\"].values.reshape(-1, 1))\n",
    "\n",
    "        look_back = 12  # Número de pasos anteriores a considerar\n",
    "        X, y = ClimaticPark.create_sequences(data.values, look_back)\n",
    "\n",
    "        # Dividimos en entrenamiento y prueba\n",
    "        train_size = int(len(X) * lr)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "        # Redimensionamos las entradas para LSTM [samples, time steps, features]\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        self._last_sequence = X_test[-1].flatten() \n",
    "        \n",
    "        model = None\n",
    "        # Cargamos o entrenamos el modelo\n",
    "        if os.path.exists(model_path) and not refresh_model:\n",
    "            print(f\"\\n\\tLoading ambient temperature predictor from {model_path}...\", end=\"\")\n",
    "            model = load_model(model_path)\n",
    "            print(\"DONE!\")\n",
    "        else:\n",
    "            print(f\"\\n\\tTraining and saving model in {model_path}...\")\n",
    "\n",
    "            _verbose= 0\n",
    "            if show_details:\n",
    "              _verbose= 1\n",
    "            # Configuración de EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "                patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "                restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "                verbose=_verbose           # Mostrar mensajes\n",
    "            )\n",
    "\n",
    "\n",
    "            # Construcción del modelo LSTM\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(50, input_shape=(look_back, 1)))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "            # Entrenamiento con EarlyStopping\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=1000,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping],  # Incluir el callback\n",
    "                verbose=1\n",
    "            )\n",
    "            self._last_sequence = X_test[-1].flatten() \n",
    "\n",
    "        model.save(model_path)\n",
    "\n",
    "        self._model= model\n",
    "        self._scaler= scaler\n",
    "\n",
    "    def predict_ambient_temperature(self, n_days_ahead):\n",
    "\n",
    "        future_predictions = []\n",
    "        current_sequence = self._last_sequence.copy()\n",
    "\n",
    "        for _ in range(n_days_ahead*24):\n",
    "            # Redimensionar la secuencia actual para que sea compatible con el modelo\n",
    "            input_data = np.array(current_sequence).reshape((1, len(current_sequence), 1))\n",
    "            \n",
    "            # Predecir el siguiente valor\n",
    "            next_pred = self._model.predict(input_data, verbose=0)[0][0]\n",
    "            \n",
    "            # Guardar el valor predicho (desnormalizado)\n",
    "            future_predictions.append(self._scaler.inverse_transform([[next_pred]])[0][0])\n",
    "\n",
    "            # Actualizar la secuencia de entrada con la nueva predicción\n",
    "            current_sequence = np.append(current_sequence[1:], next_pred)\n",
    "\n",
    "        return future_predictions\n",
    "\n",
    "    # Función para obtener datos horarios históricos de Open-Meteo API\n",
    "    def _fetch_historical_temperature(self, start_date, end_date):\n",
    "          \"\"\"\n",
    "          Obtiene datos horarios históricos de Open-Meteo API.\n",
    "\n",
    "          Args:\n",
    "          - latitude (float): Latitud de la ubicación.\n",
    "          - longitude (float): Longitud de la ubicación.\n",
    "          - start_date (str): Fecha de inicio en formato YYYY-MM-DD.\n",
    "          - end_date (str): Fecha de fin en formato YYYY-MM-DD.\n",
    "          - parameters (list): Variables meteorológicas a consultar, ej. ['temperature_2m', 'humidity_2m'].\n",
    "\n",
    "          Returns:\n",
    "          - pd.DataFrame: Datos meteorológicos horarios como DataFrame.\n",
    "          \"\"\"\n",
    "          base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "\n",
    "          # Crear el payload para la solicitud\n",
    "          payload = {\n",
    "              \"latitude\": self.lat,\n",
    "              \"longitude\": self.lon,\n",
    "              \"start_date\": start_date,\n",
    "              \"end_date\": end_date,\n",
    "              \"hourly\": 'temperature_2m',\n",
    "              \"timezone\": \"auto\"\n",
    "          }\n",
    "\n",
    "          # Realizar la solicitud a la API\n",
    "          response = requests.get(base_url, params=payload)\n",
    "\n",
    "          if response.status_code == 200:\n",
    "              # Convertir la respuesta JSON a un DataFrame\n",
    "              data = response.json()\n",
    "              if \"hourly\" in data:\n",
    "                  df = pd.DataFrame(data[\"hourly\"])\n",
    "                  df['time'] = pd.to_datetime(df['time'])\n",
    "                  df= df.set_index('time')\n",
    "                  return df\n",
    "              else:\n",
    "                  print(\"No se encontraron datos en la respuesta.\")\n",
    "                  return pd.DataFrame()\n",
    "          else:\n",
    "              print(f\"Error en la solicitud: {response.status_code} - {response.text}\")\n",
    "              return pd.DataFrame()\n",
    "          \n",
    "\n",
    "    def _forecast_uncovered_cabin_temperatures(self, ambient_temp):\n",
    "        ambient_temp_scaled = self.temp_scaler.fit_transform(ambient_temp)\n",
    "        y_pred = self.cabin_temp_model.predict(ambient_temp_scaled)\n",
    "        y_pred_rescaled = self.cabin_temp_scaler.inverse_transform(y_pred)\n",
    "        return y_pred_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_3haCJlsN8hF"
   },
   "outputs": [],
   "source": [
    "class ClimaticPark:\n",
    "    def __init__(self, file_name_lots='data/parking_lots.geojson',\n",
    "                 file_name_roofs='data/parking_roofs.geojson',\n",
    "                 file_name_coords='data/parking_coordinates.csv',\n",
    "                 file_name_gates='data/gates_coordinates.csv',\n",
    "                 file_name_mapcenter='data/map_center_coordinates.csv',\n",
    "                 file_name_cabintem='data/historical_cabin_temp.csv',\n",
    "                 file_name_tuples='data/entry_exit_tuples.csv'):\n",
    "        \"\"\"\n",
    "        Initializes the ClimaticPark object by loading all necessary files.\n",
    "        \"\"\"\n",
    "        # Load GeoJSON files for lots and roofs\n",
    "        self.lots_data = ClimaticPark.load_geojson(file_name_lots)\n",
    "        self.roofs_data = ClimaticPark.load_geojson(file_name_roofs)\n",
    "\n",
    "        # Add 'height' column to roofs_data with a value of 1 for all rows\n",
    "        if (self.lots_data is not None) and ('height' not in self.lots_data.columns):\n",
    "            self.lots_data['height'] = 1  # Assuming a default height of 1\n",
    "\n",
    "        # Add 'height' column to roofs_data with a value of 1 for all rows\n",
    "        if (self.roofs_data is not None) and ('height' not in self.roofs_data.columns):\n",
    "            self.roofs_data['height'] = 1  # Assuming a default height of 1\n",
    "\n",
    "        # Load CSV files for coordinates, historical data, and additional data\n",
    "        self.coords_data = ClimaticPark.load_csv(file_name_coords)\n",
    "        self.gates_data = ClimaticPark.load_csv(file_name_gates)\n",
    "      \n",
    "        self.data_no_roof =  ClimaticPark.load_csv(os.path.join('data', 'cabin_temperature_no_roof.csv'))\n",
    "        self.data_no_roof['coverage']=0\n",
    "        self.data_roof =  ClimaticPark.load_csv(os.path.join('data', 'cabin_temperature_w_roof.csv'))\n",
    "        self.data_roof['coverage']=1\n",
    "\n",
    "        self.recorded_cabin_temp = pd.read_csv(file_name_cabintem, index_col=0)\n",
    "        # Convertir la columna DateTime a tipo datetime\n",
    "        self.recorded_cabin_temp['DateTime'] = pd.to_datetime(self.recorded_cabin_temp['DateTime'])\n",
    "        # Establecer la columna DateTime como índice\n",
    "        self.recorded_cabin_temp.set_index('DateTime', inplace=True)\n",
    "        # Remuestrear los datos para obtener una frecuencia de 1 hora (calculando la media)\n",
    "        self.recorded_cabin_temp = self.recorded_cabin_temp.resample('h').mean()\n",
    "\n",
    "        self.entry_exit_tuples= pd.read_csv(os.path.join('data', 'entry_exit_tuples_clean.csv'), index_col=0, dtype={'id_subject':str}, parse_dates=['date'])\n",
    "        \n",
    "        os.makedirs('_models', exist_ok=True)\n",
    "        self.cabin_temp_model = None\n",
    "        self.cabin_coverage_model= None\n",
    "        print(\"Generating Demand Module...\", end=\"\")\n",
    "        self.demand_module = DemandModule(self.entry_exit_tuples)\n",
    "        print(\"DONE!\")\n",
    "        \n",
    "        lat = self.coords_data['latitude'].iloc[0]  \n",
    "        lon = self.coords_data['longitude'].iloc[0]\n",
    "        \n",
    "        print(\"Generating Shadow Module...\", end=\"\")\n",
    "        self.shadow_module = ShadowModule(lat, lon, self.roofs_data, self.lots_data)\n",
    "        print(\"DONE!\")\n",
    "\n",
    "        print(\"Generating Ambient Module...\", end=\"\")\n",
    "        #self.ambient_module = AmbientModule(lat,lon)\n",
    "        print(\"DONE!\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_geojson(file_name):\n",
    "        \"\"\"\n",
    "        Loads a GeoJSON file into a GeoDataFrame.\n",
    "        \"\"\"\n",
    "        if file_name:\n",
    "            return gpd.read_file(file_name).set_geometry(\"geometry\")\n",
    "        else:\n",
    "            print(f\"No GeoJSON file provided for {file_name}.\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_csv(file_name):\n",
    "        \"\"\"\n",
    "        Loads a CSV file into a DataFrame.\n",
    "        \"\"\"\n",
    "        if file_name:\n",
    "            return pd.read_csv(file_name)\n",
    "        else:\n",
    "            print(f\"No CSV file provided for {file_name}.\")\n",
    "            return None\n",
    "\n",
    "    def prepare_simulation(self, lr=0.8, display_details=False):\n",
    "        print(\"Preparing simulation for TPL...\")\n",
    "        \"\"\"\n",
    "        if not self.coords_data.empty:\n",
    "            latitude = self.coords_data['latitude'].iloc[0]  # Get the first coordinate\n",
    "            longitude = self.coords_data['longitude'].iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"The coordinates file is empty or not formatted correctly.\")\n",
    "        \"\"\"\n",
    "        init_date =  self.recorded_cabin_temp.index[0].date()\n",
    "        final_date =  self.recorded_cabin_temp.index[-1].date()\n",
    "\n",
    "        # Process temperature data\n",
    "        print(\"\\tTraining ambient temperature predictors...\",end=\"\")\n",
    "        #self.ambient_module.train_ambient_temperature_model(init_date, final_date)\n",
    "        #combined_temp_df= pd.concat([ambient_temp_df, self.recorded_cabin_temp], axis=1)\n",
    "        #combined_temp_df = combined_temp_df.dropna()\n",
    "\n",
    "\n",
    "        #print(\"Training cabin temperature predictors...\",end=\"\")\n",
    "        #self.cabin_temp_model, self.temp_scaler, self.cabin_temp_scaler = self._train_cabin_temperature_model(combined_temp_df, lr, display_details)\n",
    "        #self.cabin_coverage_model= self._train_cabin_temperature_and_coverage_model()\n",
    "        #print(\"DONE!\")\n",
    "\n",
    "        print(\"\\tTraining demand predictors...\",end=\"\")\n",
    "        self.demand_module.train_demand_predictors()  \n",
    "\n",
    "        print(\"Simulation ready to go!!\")\n",
    "\n",
    "    def launch_simulation(self, n_days_ahead, display_details=True):\n",
    "        \"\"\"\n",
    "        Lauch simulation for n_days_ahead \n",
    "        \"\"\"\n",
    "        print(\"Starting simulation of TPL...\")\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.coords_data.empty:\n",
    "            latitude = self.coords_data['latitude'].iloc[0]  # Get the first coordinate\n",
    "            longitude = self.coords_data['longitude'].iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"The coordinates file is empty or not formatted correctly.\")\n",
    "        \"\"\"\n",
    "\n",
    "        self.demand_module.generate_occupancy(n_days_ahead)\n",
    "        \n",
    "        init_day = self.entry_exit_tuples['date'].max().date()\n",
    "\n",
    "        date_lst = [init_day + timedelta(days=i) for i in range(n_days_ahead)]\n",
    "        print(date_lst)\n",
    "\n",
    "\n",
    "        #simulated_ambient_temp= self.ambient_module.predict_ambient_temperature(n_days_ahead) \n",
    "        #print(len(simulated_ambient_temp), simulated_ambient_temp)\n",
    "\n",
    "\n",
    "        simulated_occupancy = self.demand_module.generate_occupancy(n_days_ahead)    \n",
    "        #print(len(simulated_occupancy), simulated_occupancy)    \n",
    "\n",
    "        simulated_shadows= self.shadow_module.compute_coverage_rates(date_lst)\n",
    "        print(simulated_shadows)\n",
    "\n",
    "    # Preparamos el dataset para secuencias\n",
    "    @staticmethod\n",
    "    def create_sequences(data, look_back):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - look_back):\n",
    "            X.append(data[i:i + look_back])  # Secuencias de la variable 'a'\n",
    "            y.append(data[i + look_back])  # Predicción futura de la variable 'b'\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def _train_cabin_temperature_and_coverage_model(self):\n",
    "\n",
    "        temp_data = pd.concat([self.data_no_roof, self.data_roof], axis=0)\n",
    "\n",
    "        X= temp_data['T temp_ext coverage'.split()].values\n",
    "        y=  temp_data['temp_int'].values\n",
    "\n",
    "        clf = LinearRegression().fit(X, y)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "okbjHghGOYix",
    "outputId": "c4f6263c-ddf9-4dfa-d05f-59486896d7c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Demand Module...DONE!\n",
      "Generating Shadow Module...DONE!\n",
      "Generating Ambient Module...DONE!\n",
      "Preparing simulation for TPL...\n",
      "\tTraining ambient temperature predictors...\tTraining demand predictors...\n",
      "\tLoading demand predictor from _models\\demand_cnnlstm_model.keras...DONE!\n",
      "Simulation ready to go!!\n"
     ]
    }
   ],
   "source": [
    "park = ClimaticPark() # default parameters\n",
    "park.prepare_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation of TPL...\n",
      "[datetime.date(2022, 10, 27), datetime.date(2022, 10, 28), datetime.date(2022, 10, 29), datetime.date(2022, 10, 30)]\n",
      "                                              geometry  \\\n",
      "0    POLYGON ((-1.18571 37.99249, -1.18573 37.99248...   \n",
      "1    POLYGON ((-1.18567 37.99250, -1.18570 37.99249...   \n",
      "2    POLYGON ((-1.18564 37.99251, -1.18567 37.99250...   \n",
      "3    POLYGON ((-1.18561 37.99252, -1.18564 37.99251...   \n",
      "4    POLYGON ((-1.18558 37.99253, -1.18561 37.99252...   \n",
      "..                                                 ...   \n",
      "204  POLYGON ((-1.18530 37.99281, -1.18532 37.99280...   \n",
      "205  POLYGON ((-1.18526 37.99282, -1.18529 37.99281...   \n",
      "206  POLYGON ((-1.18523 37.99283, -1.18526 37.99282...   \n",
      "207  POLYGON ((-1.18520 37.99284, -1.18523 37.99283...   \n",
      "208  POLYGON ((-1.18517 37.99285, -1.18520 37.99284...   \n",
      "\n",
      "     coverage_rate_2022-10-27 06:00  coverage_rate_2022-10-27 07:00  \\\n",
      "0                          0.538525                        0.538605   \n",
      "1                          0.517352                        0.517432   \n",
      "2                          0.538868                        0.538948   \n",
      "3                          0.540778                        0.540859   \n",
      "4                          0.543522                        0.543602   \n",
      "..                              ...                             ...   \n",
      "204                        1.000000                        1.000000   \n",
      "205                        1.000000                        1.000000   \n",
      "206                        1.000000                        1.000000   \n",
      "207                        1.000000                        1.000000   \n",
      "208                        0.000000                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-27 08:00  coverage_rate_2022-10-27 09:00  \\\n",
      "0                          0.538685                        0.538765   \n",
      "1                          0.517513                        0.517593   \n",
      "2                          0.539028                        0.539109   \n",
      "3                          0.540939                        0.541019   \n",
      "4                          0.543682                        0.543762   \n",
      "..                              ...                             ...   \n",
      "204                        1.000000                        1.000000   \n",
      "205                        1.000000                        1.000000   \n",
      "206                        1.000000                        1.000000   \n",
      "207                        1.000000                        1.000000   \n",
      "208                        0.000000                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-27 10:00  coverage_rate_2022-10-27 11:00  \\\n",
      "0                          0.538845                        0.538925   \n",
      "1                          0.517673                        0.517753   \n",
      "2                          0.539189                        0.539269   \n",
      "3                          0.541099                        0.541179   \n",
      "4                          0.543842                        0.543922   \n",
      "..                              ...                             ...   \n",
      "204                        1.000000                        1.000000   \n",
      "205                        1.000000                        1.000000   \n",
      "206                        1.000000                        1.000000   \n",
      "207                        1.000000                        1.000000   \n",
      "208                        0.000000                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-27 12:00  coverage_rate_2022-10-27 13:00  \\\n",
      "0                          0.539005                        0.539085   \n",
      "1                          0.517833                        0.517913   \n",
      "2                          0.539349                        0.539429   \n",
      "3                          0.541259                        0.541339   \n",
      "4                          0.544002                        0.544082   \n",
      "..                              ...                             ...   \n",
      "204                        1.000000                        1.000000   \n",
      "205                        1.000000                        1.000000   \n",
      "206                        1.000000                        1.000000   \n",
      "207                        1.000000                        1.000000   \n",
      "208                        0.000000                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-27 14:00  ...  coverage_rate_2022-10-30 12:00  \\\n",
      "0                          0.539165  ...                        0.544645   \n",
      "1                          0.517993  ...                        0.523478   \n",
      "2                          0.539509  ...                        0.544988   \n",
      "3                          0.541419  ...                        0.546898   \n",
      "4                          0.544162  ...                        0.549640   \n",
      "..                              ...  ...                             ...   \n",
      "204                        1.000000  ...                        1.000000   \n",
      "205                        1.000000  ...                        1.000000   \n",
      "206                        1.000000  ...                        1.000000   \n",
      "207                        1.000000  ...                        1.000000   \n",
      "208                        0.000000  ...                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-30 13:00  coverage_rate_2022-10-30 14:00  \\\n",
      "0                          0.544721                        0.544798   \n",
      "1                          0.523554                        0.523631   \n",
      "2                          0.545064                        0.545141   \n",
      "3                          0.546974                        0.547051   \n",
      "4                          0.549717                        0.549793   \n",
      "..                              ...                             ...   \n",
      "204                        1.000000                        1.000000   \n",
      "205                        1.000000                        1.000000   \n",
      "206                        1.000000                        1.000000   \n",
      "207                        1.000000                        1.000000   \n",
      "208                        0.000000                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-30 15:00  coverage_rate_2022-10-30 16:00  \\\n",
      "0                          0.544874                        0.544951   \n",
      "1                          0.523708                        0.523784   \n",
      "2                          0.545218                        0.545294   \n",
      "3                          0.547128                        0.547204   \n",
      "4                          0.549870                        0.549946   \n",
      "..                              ...                             ...   \n",
      "204                        1.000000                        1.000000   \n",
      "205                        1.000000                        1.000000   \n",
      "206                        1.000000                        1.000000   \n",
      "207                        1.000000                        1.000000   \n",
      "208                        0.000000                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-30 17:00  coverage_rate_2022-10-30 18:00  \\\n",
      "0                          0.545027                        0.545104   \n",
      "1                          0.523861                        0.523937   \n",
      "2                          0.545371                        0.545447   \n",
      "3                          0.547281                        0.547357   \n",
      "4                          0.550023                        0.550099   \n",
      "..                              ...                             ...   \n",
      "204                        1.000000                        1.000000   \n",
      "205                        1.000000                        1.000000   \n",
      "206                        1.000000                        1.000000   \n",
      "207                        1.000000                        1.000000   \n",
      "208                        0.000000                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-30 19:00  coverage_rate_2022-10-30 20:00  \\\n",
      "0                          0.545180                        0.545257   \n",
      "1                          0.524014                        0.524090   \n",
      "2                          0.545523                        0.545600   \n",
      "3                          0.547433                        0.547510   \n",
      "4                          0.550176                        0.550252   \n",
      "..                              ...                             ...   \n",
      "204                        1.000000                        1.000000   \n",
      "205                        1.000000                        1.000000   \n",
      "206                        1.000000                        1.000000   \n",
      "207                        1.000000                        1.000000   \n",
      "208                        0.000000                        0.000000   \n",
      "\n",
      "     coverage_rate_2022-10-30 21:00  \n",
      "0                          0.545333  \n",
      "1                          0.524167  \n",
      "2                          0.545676  \n",
      "3                          0.547586  \n",
      "4                          0.550328  \n",
      "..                              ...  \n",
      "204                        1.000000  \n",
      "205                        1.000000  \n",
      "206                        1.000000  \n",
      "207                        1.000000  \n",
      "208                        0.000000  \n",
      "\n",
      "[209 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "park.launch_simulation(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9eB0DJwZqK0N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nraw_park_records_df= pd.read_csv(os.path.join('data', 'entry_exit_tuples_clean.csv'), index_col=0,\\n                              dtype={'id_subject':str}, parse_dates=['date'])\\nraw_park_records_df.head()\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "raw_park_records_df= pd.read_csv(os.path.join('data', 'entry_exit_tuples_clean.csv'), index_col=0,\n",
    "                              dtype={'id_subject':str}, parse_dates=['date'])\n",
    "raw_park_records_df.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ss7TwqaHPZyM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninit_day= raw_park_records_df['date'].max()\\nprint(init_day)\\nn_days = 4\\ndate_lst = [init_day + timedelta(days=i) for i in range(n_days)]\\ndays_lst= [date.date() for date in date_lst]\\n\\nprint(days_lst)\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "init_day= raw_park_records_df['date'].max()\n",
    "print(init_day)\n",
    "n_days = 4\n",
    "date_lst = [init_day + timedelta(days=i) for i in range(n_days)]\n",
    "days_lst= [date.date() for date in date_lst]\n",
    "\n",
    "print(days_lst)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vJlj2AcmdH5i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_components = 2  # Número de componentes de la mezcla\\ngm = GaussianMixture(n_components=n_components, random_state=42)\\ngm.fit(raw_park_records_df[['enter_hour', 'exit_hour']])\\n\\nnew_samples, _ = gm.sample(n_samples=100)\\n[[round(x[0]),round(x[1])] for x in new_samples]\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_components = 2  # Número de componentes de la mezcla\n",
    "gm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "gm.fit(raw_park_records_df[['enter_hour', 'exit_hour']])\n",
    "\n",
    "new_samples, _ = gm.sample(n_samples=100)\n",
    "[[round(x[0]),round(x[1])] for x in new_samples]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "b13rN6MlMOGm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_incoming_veh_df= raw_park_records_df.groupby(\\'date\\').size().reset_index()\\n# Crear columna datetime a partir de \\'date\\' y \\'enter_hour\\'\\nn_incoming_veh_df[\\'datetime\\'] = pd.to_datetime(n_incoming_veh_df[\\'date\\'])\\n\\n# Establecer \\'datetime\\' como índice\\nn_incoming_veh_df.set_index(\\'datetime\\', inplace=True)\\n\\n# Renombrar la columna \\'num_vehicles\\'\\nn_incoming_veh_df.rename(columns={0: \"num_vehicles\"}, inplace=True)\\n\\n# Eliminar columnas originales si ya no se necesitan\\nn_incoming_veh_df.drop(columns=[\"date\"], inplace=True)\\nn_incoming_veh_df\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_incoming_veh_df= raw_park_records_df.groupby('date').size().reset_index()\n",
    "# Crear columna datetime a partir de 'date' y 'enter_hour'\n",
    "n_incoming_veh_df['datetime'] = pd.to_datetime(n_incoming_veh_df['date'])\n",
    "\n",
    "# Establecer 'datetime' como índice\n",
    "n_incoming_veh_df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Renombrar la columna 'num_vehicles'\n",
    "n_incoming_veh_df.rename(columns={0: \"num_vehicles\"}, inplace=True)\n",
    "\n",
    "# Eliminar columnas originales si ya no se necesitan\n",
    "n_incoming_veh_df.drop(columns=[\"date\"], inplace=True)\n",
    "n_incoming_veh_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "vNIvSE1PM5IM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nax=n_incoming_veh_df.plot(figsize=(15,5), grid=True)\\nax.set_xlabel('Date',fontsize=20);\\nax.set_ylabel('Num. of hourly users',fontsize=20);\\nax.tick_params(axis='y', labelsize=15)\\nax.tick_params(axis='x', labelsize=15)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ax=n_incoming_veh_df.plot(figsize=(15,5), grid=True)\n",
    "ax.set_xlabel('Date',fontsize=20);\n",
    "ax.set_ylabel('Num. of hourly users',fontsize=20);\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tFXCAlxhQtOT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import train_test_split\\n\\nscaler = MinMaxScaler()\\nn_incoming_veh_df[\\'num_vehicles\\'] = scaler.fit_transform(n_incoming_veh_df[[\\'num_vehicles\\']])\\n\\n# Crear los datos de entrada y salida para la serie temporal\\ndef create_sequences(data, sequence_length):\\n    X, y = [], []\\n    for i in range(len(data) - sequence_length):\\n        X.append(data[i:i + sequence_length])\\n        y.append(data[i + sequence_length])\\n    return np.array(X), np.array(y)\\n\\nsequence_length = 12  # Longitud de las secuencias\\nvalues = n_incoming_veh_df[\\'num_vehicles\\'].values\\nX, y = create_sequences(values, sequence_length)\\n\\n# Dividir los datos en conjunto de entrenamiento y prueba\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\\n\\n# Definir el modelo CNN-LSTM\\nmodel = tf.keras.Sequential([\\n    tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation=\\'relu\\', input_shape=(X_train.shape[1], 1)),\\n    tf.keras.layers.MaxPooling1D(pool_size=2),\\n    tf.keras.layers.LSTM(50, activation=\\'relu\\', return_sequences=False),\\n    tf.keras.layers.Dense(1)\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'mse\\')\\n\\n# Configuración de EarlyStopping\\nearly_stopping = EarlyStopping(\\n    monitor=\"val_loss\",  # Métrica que se monitorea\\n    patience=10,         # Número de épocas de espera sin mejoras antes de detener\\n    restore_best_weights=True,  # Restaurar los mejores pesos\\n    verbose=1          # Mostrar mensajes\\n)\\n\\n# Entrenar el modelo\\n#history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\\n\\n# Entrenamiento con EarlyStopping\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    epochs=100,\\n    batch_size=16,\\n    validation_data=(X_test, y_test),\\n    callbacks=[early_stopping],  # Incluir el callback\\n    verbose=1\\n)\\n\\n\\n# Evaluar el modelo\\nloss = model.evaluate(X_test, y_test)\\nprint(f\"Pérdida en el conjunto de prueba: {loss}\")\\n\\n# Hacer predicciones\\ny_pred = model.predict(X_test)\\n\\n# Desescalar los resultados para obtener valores originales\\ny_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\\ny_pred_rescaled = scaler.inverse_transform(y_pred)\\n\\nprint(f\"Valores originales predichos: {y_pred_rescaled[:5].flatten()}\")\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "n_incoming_veh_df['num_vehicles'] = scaler.fit_transform(n_incoming_veh_df[['num_vehicles']])\n",
    "\n",
    "# Crear los datos de entrada y salida para la serie temporal\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 12  # Longitud de las secuencias\n",
    "values = n_incoming_veh_df['num_vehicles'].values\n",
    "X, y = create_sequences(values, sequence_length)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cambiar la forma de los datos para adaptarse a la entrada CNN-LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Definir el modelo CNN-LSTM\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.LSTM(50, activation='relu', return_sequences=False),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Configuración de EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Métrica que se monitorea\n",
    "    patience=10,         # Número de épocas de espera sin mejoras antes de detener\n",
    "    restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "    verbose=1          # Mostrar mensajes\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "#history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Entrenamiento con EarlyStopping\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],  # Incluir el callback\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Pérdida en el conjunto de prueba: {loss}\")\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Desescalar los resultados para obtener valores originales\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
    "\n",
    "print(f\"Valores originales predichos: {y_pred_rescaled[:5].flatten()}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Mcuemtgobikt"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3025937763.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[34], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    Obtiene datos horarios históricos de Open-Meteo API.\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Función para obtener datos horarios históricos de Open-Meteo API\n",
    "def get_historical_weather_hourly(latitude, longitude, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Obtiene datos horarios históricos de Open-Meteo API.\n",
    "\n",
    "    Args:\n",
    "    - latitude (float): Latitud de la ubicación.\n",
    "    - longitude (float): Longitud de la ubicación.\n",
    "    - start_date (str): Fecha de inicio en formato YYYY-MM-DD.\n",
    "    - end_date (str): Fecha de fin en formato YYYY-MM-DD.\n",
    "    - parameters (list): Variables meteorológicas a consultar, ej. ['temperature_2m', 'humidity_2m'].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Datos meteorológicos horarios como DataFrame.\n",
    "    \"\"\"\n",
    "    base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "\n",
    "    # Crear el payload para la solicitud\n",
    "    payload = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": 'temperature_2m',\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "\n",
    "    # Realizar la solicitud a la API\n",
    "    response = requests.get(base_url, params=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Convertir la respuesta JSON a un DataFrame\n",
    "        data = response.json()\n",
    "        if \"hourly\" in data:\n",
    "            df = pd.DataFrame(data[\"hourly\"])\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No se encontraron datos en la respuesta.\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"Error en la solicitud: {response.status_code} - {response.text}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "ambient_temperatures_df = get_historical_weather_hourly(park.coords_data['latitude'].iloc[0], park.coords_data['longitude'].iloc[0], init_date, final_date)\n",
    "ambient_temperatures_df\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "climaticpark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
